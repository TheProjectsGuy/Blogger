{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A perceptron model, sometimes also referred as a single neuron neural network or a linear learning model. It basically performs a weighed sum of the input with weights, adds a bias and then passes the value through an _activation function_. \n",
    "In short, the steps to get an output are\n",
    "1. Perform weighed sum and add bias\n",
    "2. Pass the obtained value through a function\n",
    "\n",
    "We'll use the famous _sigmoid_ activation function here, where\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Therefore, to summarize. We'll do the following in the right order to forward propagate. (This is what a perceptron does)\n",
    "\n",
    "$$Z = W*X + b$$\n",
    "$$A = sigmoid(Z)$$\n",
    "\n",
    "Here, *X* is an input vector (column type), *W* is the weights vector (row type) and *b* is the bias.\n",
    "\n",
    "### _Hand Gesture Recognition_\n",
    "Here are the different hand gestures for this purpose. We have another gesture (gesture number 0) which shows no fingers (just a closed fist) and is not shown here\n",
    "![Hand Gestures](Hand-Gestures-count.jpg \"Hand Gestures in the database\")\n",
    "\n",
    "For now we will consider the gesture number **5**, and we'll train a perceptron to recognize that gesture with a descent accuracy. In total we have 6 gestures\n",
    "\n",
    "Let's start by importing the files we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data we need\n",
    "We have approximately 231 training examples from each gesture type\n",
    "This data is in form of [235, 190] images which are shrunk to [47, 38] images using image interpolation, this is done because otherwise there will be too many parameters for the perceptron to deal with. The data acquisition process was as following\n",
    "1. Get black and white binary images of shape [235, 190]\n",
    "2. Use interpolation and shrink them to shape [47, 38]\n",
    "3. Flatten every image into a column vector and stack all those column vectors together\n",
    "\n",
    "There are many techniques to generate more data from the data that you already have. These techniques are called _data augmentation_.\n",
    "\n",
    "Fortunately, all this work has already been done for us and the variables are stored in binary files. We only have to load them into memory. Additionally the binary images have two values, 0 and 255 (because the were 8 bit numbers). We will scale them down to the range 0 to 1. This makes the inputs smaller and all of them will be in range. This technique is called _data normalization_ and increases the efficiency of the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the data variables from memory into the program\n",
    "def load_data_variables(output_number = 5, normalize_inputs = True, view_sample_data = True):\n",
    "    # Load 47 x 38 images (input data)\n",
    "    X = np.load('Data/X.npy')\n",
    "    # Load one hot encoded output data\n",
    "    Y_one_hot = np.load('Data/Y_one_hot_encoded.npy')\n",
    "    # Get the 5th row (gesture number 5)\n",
    "    Y = np.array(Y_one_hot[output_number,:])\n",
    "    # Assert it to be a row vector\n",
    "    Y = Y.reshape((1,-1))\n",
    "    # Viewing data sample\n",
    "    if view_sample_data:\n",
    "        print(\"DATA DEBUG : Viewing 100 examples\")\n",
    "        view_100_examples(X)\n",
    "    # Normalize data\n",
    "    if normalize_inputs:\n",
    "        X = X/255\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write some code to display a 10 x 10 image sample using matplotlib. Note that _Greys_ will display inverted binary images (swap 0 and 1), this is done just to suit the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show input samples\n",
    "def view_100_examples(X):\n",
    "    # View 100 random 47 x 38 images from the dataset\n",
    "    for i in range(1, 101):\n",
    "        rno = np.random.randint(X.shape[1])\n",
    "        img = X[:,rno].reshape(47, 38)\n",
    "        plt.subplot(10, 10, i)\n",
    "        plt.imshow(img, cmap='Greys')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Random Images\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now finally load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading variables\")\n",
    "X, Y = load_data_variables()\n",
    "print(\"DATA INFO : Variables loaded, input shape is {x_shape}, output shape is {y_shape}\".format(\n",
    "    x_shape=X.shape, y_shape= Y.shape\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output : \n",
    "\n",
    "Feature | Shape\n",
    "-------|------\n",
    "input shape | (1786, 2772)\n",
    "output shape | (1, 2772)\n",
    "\n",
    "This is because we have 231 training examples of each gesture and we have 6 gestures. Now that gets 231 * 6 = 1386 training examples. But since we performed data augmentation we have 2 images for one input (flipped versions). Thus we have 1386 * 2 = 2772 training examples. Input has 47 * 38 = 1786 fields (rows) and output has just one row. So numbers match. Let's move forward with shuffling this data. This is a very good practice to breat a biased distribution from a source. This always let's us \"learn\" better from our data. We will just shuffle the columns in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "def shuffle_data(X, Y, iternum = 1):\n",
    "    print(\"DATA DEBUG : Shuffling data for {num_iter} iterations\".format(num_iter=iternum))\n",
    "    # Make a dummy matrix\n",
    "    sh_mat = X\n",
    "    # Row stack the output matrix\n",
    "    sh_mat = np.row_stack((sh_mat, Y))\n",
    "    sh_mat = sh_mat.T   # Because the np.random.shuffle shuffle's rows\n",
    "    for i in range(iternum):\n",
    "        np.random.shuffle(sh_mat)\n",
    "    sh_mat = sh_mat.T  # Get back the data into column form\n",
    "    X = sh_mat[0:X.shape[0], :]                            # Shuffled input data\n",
    "    Y = sh_mat[X.shape[0]:X.shape[0] + Y.shape[0], :]      # Shuffled output data\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = shuffle_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "We now need to split the data into three sets (three buctets). Namely, split the data into **train**, **dev** and **test** sets. <br>\n",
    "Why do that ? <br>\n",
    "Because it rarely happens that we try out only one type of model and that is the best choice. We take many models into consideration. For instance, I might want to know that how does a different activation function affect the final accuracy. So we have one metric to decide the performance of our ML model. We call this the _performance metric_. So we train our models on the train set, compare different models using the dev set. The test set is more like a final run performance. Once we've chosen the best model, we want to know how our model will perform on real world data. For this, we have to check performance on the data that the model hasn't seen yet, hence the test set. Formally, \n",
    "\n",
    "Set | Purpose\n",
    "----|----\n",
    "Train | To train the model\n",
    "Dev | To compare different models\n",
    "Test | To test the final performance of our model\n",
    "\n",
    "Sometimes, the dev set is also called the \"cross validation\" set.<br>\n",
    "Now how do we choose the distribution ratio ?<br>\n",
    "Well, we aim to put most of the data into the training bucket, and a few amount in dev and test sets. Another important thing is that we need the dev and test sets to be from nearly the same probability distribution. This is so that we can choose the most robust model and also gain some knowledge about the changes we have to make to the model. The performance metric gives us insight into our model, if it's overfitting our data (causing too much variance) or underfitting our data, causing too much bias. We'll deal with _bias, variance tradeoff_ later. For now, let's go with 2500 training, 136 dev and test set examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_dev_test(X, Y, sizes=None, shuffle=False):\n",
    "    # Split given data into train, dev and test sets\n",
    "    if sizes is None:\n",
    "        # Default sizes\n",
    "        size_train = 2500\n",
    "        size_dev = 136\n",
    "        size_test = 136\n",
    "    else:\n",
    "        (size_train, size_dev, size_test) = sizes\n",
    "        assert (size_train + size_test + size_dev <= X.shape[1]), \"Size mismatch error {v1}>{v2}\".format(\n",
    "            v1=size_train + size_test + size_dev, v2=X.shape[1]\n",
    "        )\n",
    "    if shuffle:\n",
    "        X, Y = shuffle_data(X, Y)\n",
    "    x_train = np.array(X[:,:size_train])\n",
    "    print(\"DATA DEBUG : x_train is of shape {x_train_shape}\".format(\n",
    "        x_train_shape=x_train.shape\n",
    "    ))\n",
    "    y_train = np.array(Y[:,:size_train])\n",
    "    print(\"DATA DEBUG : y_train is of shape {y_train_shape}\".format(\n",
    "        y_train_shape=y_train.shape\n",
    "    ))\n",
    "    x_dev = np.array(X[:,size_train: size_train + size_dev])\n",
    "    print(\"DATA DEBUG : x_dev is of shape {x_dev_shape}\".format(\n",
    "        x_dev_shape=x_dev.shape\n",
    "    ))\n",
    "    y_dev = np.array(Y[:, size_train: size_train + size_dev])\n",
    "    print(\"DATA DEBUG : y_dev is of shape {y_dev_shape}\".format(\n",
    "        y_dev_shape=y_dev.shape\n",
    "    ))\n",
    "    x_test = np.array(X[:, size_train + size_dev: size_train + size_dev + size_test])\n",
    "    print(\"DATA DEBUG : x_test is of shape {x_test_shape}\".format(\n",
    "        x_test_shape=x_test.shape\n",
    "    ))\n",
    "    y_test = np.array(Y[:, size_train + size_dev: size_train + size_dev + size_test])\n",
    "    print(\"DATA DEBUG : y_test is of shape {y_test_shape}\".format(\n",
    "        y_test_shape=y_test.shape\n",
    "    ))\n",
    "    rdict = {\n",
    "        \"train\": (x_train, y_train),\n",
    "        \"dev\": (x_dev, y_dev),\n",
    "        \"test\": (x_test, y_test)\n",
    "    }\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = generate_train_dev_test(X, Y)\n",
    "(X_train, Y_train) = data_sets[\"train\"]\n",
    "(X_dev, Y_dev) = data_sets[\"dev\"]\n",
    "(X_test, Y_test) = data_sets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "Set | Shape\n",
    "------|------\n",
    "X-train | (1786, 2500)\n",
    "Y-train | (1, 2500)\n",
    "X-dev | (1786, 136)\n",
    "Y-dev | (1, 136)\n",
    "X-test | (1786, 136)\n",
    "Y-test | (1, 136)\n",
    "\n",
    "## Main model\n",
    "Now, we start the main model code\n",
    "Remember, our model does a weighed sum and then passes it through sigmoid function. This is sometimes also written as **Linear -> Sigmoid**. The maths on this is shown below\n",
    "![Perceptron math](IMG_0233.JPG \"Mathematical derivation\")\n",
    "\n",
    "The two main parts are in black. Now they are for row major data, we're dealing with column major (so change axis of summation). We'll discuss this when the time to program the model comes\n",
    "\n",
    "For now, let's initialize parameters \"W\" and \"b\". Remember that **W** is (1, 1786) and **b** is (1, 1).\n",
    "The main question is, what do we initialize the parameters to ?<br>\n",
    "Well, if we initialize all the weights to zero and the bias to zero, then we will predict zero as output. In case of perceptron, this might work sometimes, but this is a bad practice, because when we have a bigger model, initializing all weights to zeros will end up in all weights learning the same thing. This dampens the learning of our model and significantly reduces the performance. For now, we initialize the weights with random numbers. The method I used is called _He initialization_. For the bias, it doesn't matter if it's initialized to 0 or not, it's just one value.\n",
    "\n",
    "The code behind this is\n",
    "```python\n",
    "params[\"W\"] = np.random.rand(1, n) * (2/np.sqrt(n-1))\n",
    "params[\"b\"] = np.zeros((1,1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, Y, seedval=2):\n",
    "    # Initialize parameters\n",
    "    n = X.shape[0]      # Number of fields\n",
    "    def initialize_zeros():\n",
    "        W = np.zeros((1, n))\n",
    "        b = np.zeros((1, 1))\n",
    "        rdict = {\n",
    "            \"W\": W,\n",
    "            \"b\": b\n",
    "        }\n",
    "        return rdict\n",
    "    params = initialize_zeros()\n",
    "    np.random.seed(seedval)\n",
    "    W = params['W']\n",
    "    b = params['b']\n",
    "    # Initialize weights\n",
    "    W = np.random.rand(*W.shape) * (2/np.sqrt(n - 1))\n",
    "    # Initialize bias\n",
    "    b = np.zeros(b.shape)\n",
    "    print(\"DATA DEBUG : Weights of shape {w_shape}, sample is {w_sample}\".format(\n",
    "        w_shape=W.shape, w_sample=W\n",
    "    ))\n",
    "    print(\"DATA DEBUG : Bias of shape {b_shape}, sample is {b_sample}\".format(\n",
    "        b_shape=b.shape, b_sample=b\n",
    "    ))\n",
    "    params = {\n",
    "        \"W\": W,\n",
    "        \"b\":b\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = initialize_parameters(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "Parameter | Shape | Initial value\n",
    "------|--------|------\n",
    "\"W\"| (1, 1786) | [[0.02063917 0.0012273 ... 0.02258119 0.00640937]]\n",
    "\"b\"| (1, 1) | [[0.]]\n",
    "\n",
    "Now that we have initialized the parameters, let us declare the sigmoid function and it's derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid functions\n",
    "def sigmoid(x):\n",
    "    # Return sigmoid of an input\n",
    "    return np.exp(x)/(1 + np.exp(x))\n",
    "# Sigmoid derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST : Sigmoid = {sgm_res}\".format(sgm_res=sigmoid(0)))\n",
    "print(\"TEST : Sigmoid Derivative = {sd_res}\".format(\n",
    "    sd_res=sigmoid_derivative(0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "Function | Result\n",
    "----|----\n",
    "Sigmoid | 0.5\n",
    "Sigmoid Derivative | 0.235\n",
    "\n",
    "Now, we must implement the forward propagation step. Remember that the parameter **W** is the weight of every field, so the weighed sum with bias of the entire input space can be calculated by $Z = W * X + b$. Here, matrice multiplication is used (between _W_ and _X_).\n",
    "We then obtain the output through the sigmoid function\n",
    "$$A = sigmoid(Z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation function\n",
    "def forward_propagate(params, X):\n",
    "    # Forward propagation step\n",
    "    W = params[\"W\"]\n",
    "    b = params[\"b\"]\n",
    "    Z = np.matmul(W, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate the test examples for the forward propagation step\n",
    "def generate_forward_propagation_test():\n",
    "    test_x = np.arange(6).reshape((3, 2))\n",
    "    test_W = np.array([0.2, 0.5, 0.6]).reshape((1, -1))\n",
    "    test_b = np.array([3])\n",
    "    rdict = {\n",
    "        \"W\":test_W,\n",
    "        \"b\":test_b\n",
    "    }\n",
    "    return rdict, test_x\n",
    "\n",
    "test_params, test_x = generate_forward_propagation_test()\n",
    "print(\"TEST : Forward Propagation test = {fp_res}\".format(\n",
    "    fp_res=forward_propagate(test_params, test_x)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    " Test | Result\n",
    "----|----\n",
    "Forward Propagation| [[0.9983412, 0.99954738]]\n",
    "\n",
    "### Cost function\n",
    "This is the function through which we must optimize. This is often denoted by **J**. \n",
    "We use the logistic regression cost function for classification based problems, also known as the _cross entropy cost function_.\n",
    "The cost function performed on just one training example is also called the loss function. This function actually takes in two values, the predictions *A* (or also called $\\hat{Y}$) and the actual value *Y*.\n",
    "The cross entropy loss is given by\n",
    "\n",
    "$$Loss(A,Y) = -(Y \\times ln(A) + (1-Y) \\times ln(1-A))$$\n",
    "<center>**OR**</center>\n",
    "$$Loss(\\hat{Y},Y) = -(Y \\times ln(\\hat{Y}) + (1-Y) \\times ln(1-\\hat{Y}))$$\n",
    "\n",
    "The cost is just the average value of loss over the entire training set. Hence the cross entropy cost funciton is \n",
    "$$\\Rightarrow Cross(A,Y) = \\frac{1}{m} \\sum_{k = 1}^{m} Loss(A^{k}, Y^{k})$$\n",
    "\n",
    "But, we will implement a different cost function in code. This loss function is simply given by the distance between the two vectors A and Y squared multiplied by `0.5`. Mathematically\n",
    "\n",
    "$$Cost(A, Y) = \\frac{||A - Y||^{2}}{2\\, m}$$\n",
    "\n",
    "This is being done because we've calculated optimization formula through hand in the coming cells, so we'll only be needing this for verification purposes and it's easier to interpret results through this type of cost funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def compute_cost(params, X, Y):\n",
    "    # Compute the cost\n",
    "    A = forward_propagate(params, X)\n",
    "    L = np.square(A - Y)\n",
    "    return np.average(L)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Every weight and bias is adjusted through an optimizing function. The simplest optimizing function is _gradient descent_. This algorithm takes a small step towards the minima.\n",
    "<center>**Gradient Descent algorithm**</center>\n",
    "![Gradient Descent algorithm](gradient-descent.png \"Gradient Descent\")\n",
    "\n",
    "$$W_{k} := W_{k} - \\alpha \\times  \\frac{\\partial J}{\\partial W_{k}}$$\n",
    "$$b := b - \\alpha \\times  \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "(The := means compute whatever is on the right and assign it to the left variable)<br>\n",
    "If we differentiate the _cross entropy cost function_ and apply the _chain rule of differentiation_ we get the following results\n",
    "\n",
    "$$W_{k} := W_{k} - \\alpha \\times [\\frac{1}{m} \\sum_{i = 1}^{m} (A^{i} - Y^{i}) \\times X_{k}^{i}]$$\n",
    "$$b := b - \\alpha \\times [\\frac{1}{m} \\sum_{i = 1}^{m} (A^{i} - Y^{i})]$$\n",
    "\n",
    "In case you're wondering, $X_{k}^{i}$ stands for the k<sup>th</sup> feature of i<sup>th</sup> training example. In our column major form, it's the k<sup>th</sup> row and i<sup>th</sup> column of **X**. **W**<sub>k</sub> stands for the weight of the k<sup>th</sup> feature.\n",
    "\n",
    "A vectorized implementation would we great since they're optimized. In python, we can carry this out through the following\n",
    "```python\n",
    "W = W - (learning_rate/m) * np.sum(X * (A - Y), axis=1, keepdims=True).T\n",
    "b = b - (learning_rate/m) * np.sum((A - Y), axis=1, keepdims=True).T\n",
    "```\n",
    "If you pay closer attention, it's not so difficult to understand. X * (A - Y) computes (A - Y) and then performs row wise multiplication with X, then we add all the columns. The transpose (.T) is just to convert the delta (gradient) from a column vector to a row vector, to match the dimensions of W. Same story goes with b\n",
    "\n",
    "This tells us that there's a way simpler implementation of the cost function. One which is more efficient to calculate and will do the exact same thing if we tune the optimization code as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization code\n",
    "def train_parameters_gradient_descent(params, X, Y, num_iter = 100, learning_rate = 0.01, print_cost = True):\n",
    "    # Get shape\n",
    "    (n, m) = X.shape\n",
    "    assert (m == Y.shape[1]), \"ERROR : Dataset mismatch\"\n",
    "    # Store the history of costs\n",
    "    cost_history = {\n",
    "        \"training_costs_x\" : [],   # i values to plot w.r.t\n",
    "        \"training_costs\": [],      # cost during training\n",
    "        \"test_costs_x\" : [],       # i values to plot w.r.t\n",
    "        \"test_costs\": []           # cost over the test set during training\n",
    "    }\n",
    "    # Iterate over the entire batch of data\n",
    "    for i in range(num_iter):\n",
    "        # Forward propagate to calculate prediction\n",
    "        A = forward_propagate(params, X)\n",
    "        # Get weights and biases\n",
    "        W = params[\"W\"]\n",
    "        b = params[\"b\"]\n",
    "        # Gradient Descent optimization algorithm\n",
    "        W = W - (learning_rate/m) * np.sum(X * (A - Y), axis=1, keepdims=True).T\n",
    "        b = b - (learning_rate/m) * np.sum((A - Y), axis=1, keepdims=True).T\n",
    "        # Update parameters\n",
    "        params[\"W\"] = W\n",
    "        params[\"b\"] = b\n",
    "        current_cost = compute_cost(params, X, Y)\n",
    "        # Show output after some number of iterations to report training\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"TRAINING INFO : Iteration {iter}, cost = {cst}\".format(\n",
    "                iter= i, cst=current_cost))\n",
    "            print(\"TRAINING DEBUG : W {wsh}, X {xsh}, b {bsh}, A {ash}, Y {ysh}\".format(\n",
    "                    wsh=params[\"W\"].shape, xsh=X.shape, bsh=params[\"b\"].shape,\n",
    "                    ash=A.shape, ysh= Y.shape))\n",
    "            cost_test = compute_cost(params, X_dev, Y_dev)\n",
    "            cost_history[\"test_costs\"].append(cost_test)\n",
    "            cost_history[\"test_costs_x\"].append(i + 1)\n",
    "        cost_history[\"training_costs\"].append(current_cost)\n",
    "        cost_history[\"training_costs_x\"].append(i + 1)\n",
    "    return cost_history, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've implemented everything required to train, let us train our perceptron and get the results.<br>\n",
    "**WARNING** : Run the below cell only once to get the expected output, if you want to run it again, please re initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already initialised stuff (uncomment if you feel like)\n",
    "# params = initialize_parameters(X_train, Y_train)\n",
    "# Actual training\n",
    "number_iterations = 100\n",
    "c_hist, params = train_parameters_gradient_descent(params, X_train, Y_train, num_iter=number_iterations)\n",
    "plt.plot(c_hist[\"training_costs_x\"], c_hist[\"training_costs\"], 'b-', c_hist[\"test_costs_x\"], c_hist[\"test_costs\"], 'g-')\n",
    "plt.title('Cost History')\n",
    "plt.legend(['Training','Dev'])\n",
    "plt.show()\n",
    "print(\"TRAINING DEBUG : Training done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "Iteration | Cost\n",
    "---|---\n",
    "0 | 0.4159\n",
    "10 | 0.1023\n",
    "$\\vdots$ | $\\vdots$\n",
    "90 | 0.07683\n",
    "\n",
    "The graph in the end shows us an important result. It has captured the progress of training the model. If you don't see a descent in the blue line (if it's raising), it indicates that there has been an error. We have very unprofessionally used the dev set to check the perforce while training. Usually there's a fourth set for this, it's most of the times called the 'Training_dev set'. Training and evaluation is a very important metric and is used by the best of libraries which do such kind of tasks. It enables them to gain insight into the training process.\n",
    "\n",
    "## Error analysis\n",
    "\n",
    "Now that we've trained the model, it's time we check out it's performance on the test set as well. We'll use a function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error on the test set\n",
    "def error_test_set(params, test_x, test_y, view_mismatches=True):\n",
    "    # Number of test samples\n",
    "    (_, m) = test_y.shape\n",
    "    # Get predictions through forward propagation\n",
    "    A = forward_propagate(params, test_x)\n",
    "    print(\"TEST DEBUG : W {wsh}, X {xsh}, b {bsh}, Y {ysh}\".format(\n",
    "        wsh=params[\"W\"].shape, xsh= X.shape, bsh= params[\"b\"].shape, ysh= test_y.shape\n",
    "    ))\n",
    "    # Threshold places where the confidence level is > 50 %\n",
    "    B = np.zeros_like(A)\n",
    "    B[A > 0.5] = 1\n",
    "    # Compute the mismatches and error\n",
    "    D = np.square(test_y - B)\n",
    "    D = np.array(D)\n",
    "    # Wherever D is 1, it's a mismatch\n",
    "    mismatches = D.nonzero()[1]\n",
    "    print(\"TEST DEBUG : {num_mis} mismatches (out of {num}) found\".format(\n",
    "        num_mis= mismatches.shape[0], num=test_y.shape[1]))\n",
    "    if view_mismatches == True:\n",
    "        # View mismatches \n",
    "        print(\"TEST DEBUG : Viewing mismatches\")\n",
    "        for i in range(len(mismatches)):\n",
    "            plt.imshow(test_x[:,mismatches[i]].reshape((47,38)), cmap='Greys')\n",
    "            plt.title(\"A = {a}, Y = {y}\".format(a=round(A[0, mismatches[i]], 2), y=test_y[0, mismatches[i]]))\n",
    "            plt.show()\n",
    "    error = np.sum(D)/m\n",
    "    return error * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = error_test_set(params, X_test, Y_test, view_mismatches= False)\n",
    "print(\"TEST DEBUG : Error is {err}%\".format(err=error))\n",
    "print(\"Accuracy is {acc}%\".format(acc=100-error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output \n",
    "\n",
    "Field | Value\n",
    "--|--\n",
    "Number of mismatches | $\\approx$ 22 out of 136\n",
    "Error percentage | $\\approx$ 16.17647 %\n",
    "Accuracy | $\\approx$ 83.82 %\n",
    "\n",
    "We have finally made a linear classifier that can detect a 'Five finger' gesture which has 83.8 % accuracy. This is pretty awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = error_test_set(params, X_test, Y_test, view_mismatches= True)\n",
    "print(\"TEST DEBUG : Error is {err}%\".format(err=error))\n",
    "print(\"Accuracy is {acc}%\".format(acc=100-error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above gives us even more detailed insight into our learned parameters. It shows what it's exactly predicting as 'Five finger gesture' and what it's not predicting as a 'Five finger gesture'. We can see that if we bring any kind of swirl or change the positions from the standard, we get an invalid result. This is because the perceptron cannot get that level of complexity. We cannot encode that high amount of complexity in a single neuron. However, this performance isn't very bad.\n",
    "\n",
    "Feel free to play with more models below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables and divide into test, train and dev sets\n",
    "X, Y = load_data_variables(output_number=3)\n",
    "data_sets = generate_train_dev_test(X, Y, shuffle=True)\n",
    "(X_train, Y_train) = data_sets[\"train\"]\n",
    "(X_dev, Y_dev) = data_sets[\"dev\"]\n",
    "(X_test, Y_test) = data_sets[\"test\"]\n",
    "# Initialize random parameters\n",
    "params = initialize_parameters(X_train, Y_train)\n",
    "# Train\n",
    "number_iterations = 100\n",
    "c_hist, params = train_parameters_gradient_descent(params, X_train, Y_train, num_iter=number_iterations)\n",
    "plt.plot(c_hist[\"training_costs_x\"], c_hist[\"training_costs\"], 'b-', c_hist[\"test_costs_x\"], c_hist[\"test_costs\"], 'g-')\n",
    "plt.title('Cost History')\n",
    "plt.legend(['Training','Dev'])\n",
    "plt.show()\n",
    "print(\"TRAINING DEBUG : Training done.\")\n",
    "# Error analysis\n",
    "error = error_test_set(params, X_test, Y_test, view_mismatches= True)\n",
    "print(\"TEST DEBUG : Error is {err}%\".format(err=error))\n",
    "print(\"Accuracy is {acc}%\".format(acc=100-error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results I got are\n",
    "\n",
    "Gesture | Accuracy\n",
    "--|--\n",
    "5 | 83.823 %\n",
    "4 | 81.617 %\n",
    "3 | 77.941 %\n",
    "2 | 83.088 %\n",
    "1 | 83.088 %\n",
    "0 | 86.764 %\n",
    "\n",
    "Well, this clearly isn't the best we can do. There are multiple directions to head from here\n",
    "\n",
    "- Get edges from mask and use that as training data\n",
    "- Build a more sophisticated model (like a DNN, ANN or a CNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
