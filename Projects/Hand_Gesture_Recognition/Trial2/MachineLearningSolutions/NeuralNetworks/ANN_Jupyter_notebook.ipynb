{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir_name=\"Data\", x_name=\"X.npy\",\n",
    "                 y_name=\"Y_one_hot_encoded.npy\", one_hot_index=0, shuffle_data=True, normalize_data = True):\n",
    "    \"\"\"\n",
    "    Loads a dataset stored as a .npy file\n",
    "    :param dataset_dir_name: Name of the folder in which data is\n",
    "    :param x_name: Name of file which contains inputs (with .npy extension)\n",
    "    :param y_name: Name of file which contains outputs (with .npy extension)\n",
    "    :param one_hot_index: The index you're training for (pass -1 for passing raw output file), should be >= -1\n",
    "    :param shuffle_data: Shuffle the data after getting it from file\n",
    "    :return:\n",
    "        X, Y\n",
    "        X -> Inputs\n",
    "        Y -> Outputs\n",
    "        The output will be shuffled if shuffle_data is True, else it won't be shuffled\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    X = np.load(\"../{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                 f_name=x_name))\n",
    "    print(\"DATA DEBUG : Inputs shape is {in_shape}\".format(in_shape=X.shape))\n",
    "    if one_hot_index == -1:\n",
    "        # Parse the entire dataset as it is\n",
    "        Y = np.load(\"../{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                     f_name=y_name))\n",
    "    elif one_hot_index >= 0:\n",
    "        Y_one_hot = np.load(\"../{main_root}/{f_name}\".format(\n",
    "            main_root=dataset_dir_name, f_name=y_name\n",
    "        ))\n",
    "        Y = np.array(Y_one_hot[one_hot_index, :].reshape((1, -1)))\n",
    "    else:\n",
    "        raise IndexError(\"The index {ind_number} is an illegal index\".format(ind_number=one_hot_index))\n",
    "    print(\"DATA DEBUG : Output shape is {out_shape}\".format(out_shape=Y.shape))\n",
    "    if normalize_data:\n",
    "        X = X / 255\n",
    "    # Shuffle the dataset\n",
    "    def shuffle_dataset(X, Y):\n",
    "        buffer_data = np.row_stack((X, Y))\n",
    "        buffer_data = buffer_data.T\n",
    "        np.random.shuffle(buffer_data)\n",
    "        buffer_data = buffer_data.T\n",
    "        X = buffer_data[0:X.shape[0], :]\n",
    "        Y = buffer_data[X.shape[0]:X.shape[0] + Y.shape[0], :]\n",
    "        return X, Y\n",
    "\n",
    "    if shuffle_data:\n",
    "        X, Y = shuffle_dataset(X, Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev_test(X, Y, sizes=(2500, 136, 136)):\n",
    "    \"\"\"\n",
    "    Split the dataset into train, dev and test sets\n",
    "    :param X: Inputs\n",
    "    :param Y: Outputs\n",
    "    :param sizes: Distribution tuple\n",
    "    :return: Dictionary\n",
    "        \"train\" : (X_train, Y_train)\n",
    "        \"dev\" : (X_dev, Y_dev)\n",
    "        \"test\" : (X_test, Y_test)\n",
    "    \"\"\"\n",
    "    # Split the dataset\n",
    "    X_train = X[:, 0:sizes[0]]\n",
    "    Y_train = Y[:, 0:sizes[0]]\n",
    "    X_dev = X[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    Y_dev = Y[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    X_test = X[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    Y_test = Y[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    print(\"DATA DEBUG : Train input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_train.shape, out_shape=Y_train.shape\n",
    "    ))\n",
    "    print(\"DATA DEBUG : Test input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_test.shape, out_shape=Y_test.shape\n",
    "    ))\n",
    "    print(\"DATA DEV : Dev input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_dev.shape, out_shape=Y_dev.shape\n",
    "    ))\n",
    "    rdict = {\n",
    "        \"train\": (X_train, Y_train),\n",
    "        \"dev\": (X_dev, Y_dev),\n",
    "        \"test\": (X_test, Y_test)\n",
    "    }\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_deep(input_size, layer_tup):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the DNN\n",
    "    :param input_size: Size of the input layer\n",
    "    :param layer_tup:  Tuple of layer sizes (hidden layers + output layer)\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"W + str(i)\" : Weight of layer i\n",
    "            \"b + str(i)\" : Biases of layer i\n",
    "    \"\"\"\n",
    "    # Initialize the neural network with parameters\n",
    "    layer_size = (input_size, *layer_tup)\n",
    "    print(\"DATA DEBUG : Initializing neural network with architecture {arc}\".format(arc=layer_size))\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_size)):\n",
    "        params[\"W\" + str(i)] = np.random.rand(layer_size[i], layer_size[i - 1]) * 10\n",
    "        params[\"b\" + str(i)] = np.zeros((layer_size[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_activations(activations):\n",
    "    \"\"\"\n",
    "    Parse activations into function and derivatives\n",
    "    :param activations: Function which returns a dictionary\n",
    "        \"function\" : Activation function\n",
    "        \"derivative\" : Derivative of function\n",
    "    :return:\n",
    "        activation_function, activation_function_derivatives\n",
    "    \"\"\"\n",
    "    activation_fncs = []\n",
    "    activation_fnc_der = []\n",
    "    for fnc in activations:\n",
    "        activation_fncs.append(fnc()[\"function\"])\n",
    "        activation_fnc_der.append(fnc()[\"derivative\"])\n",
    "    return activation_fncs, activation_fnc_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : sigmoid_function\n",
    "            \"derivative\" : sigmoid_derivative\n",
    "    \"\"\"\n",
    "    def sigmoid_function(x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid_function(x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid'(x)\n",
    "        \"\"\"\n",
    "        return sigmoid_function(x) * (1 - sigmoid_function(x))\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": sigmoid_function,\n",
    "        \"derivative\": sigmoid_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu():\n",
    "    \"\"\"\n",
    "    ReLU (Rectified linear unit) activation function\n",
    "    :param x:\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : relu_function\n",
    "            \"derivative\" : relu_derivative\n",
    "    \"\"\"\n",
    "    def relu_function(x):\n",
    "        \"\"\"\n",
    "        ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu(x)\n",
    "        \"\"\"\n",
    "        ret_x = x.copy()\n",
    "        ret_x[ret_x < 0] = 0\n",
    "        return ret_x\n",
    "\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu'(x)\n",
    "        \"\"\"\n",
    "        ret_d = x.copy()\n",
    "        ret_d[x <= 0] = 0\n",
    "        ret_d[x > 0] = 1\n",
    "        return ret_d\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": relu_function,\n",
    "        \"derivative\": relu_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate_deep(params, activations, input):\n",
    "    \"\"\"\n",
    "    Perform forward propagation of the neural network\n",
    "    :param params: Dictionary of weights and biases of the network\n",
    "            params[\"W\" + str(l)] : Weights of layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: Array of activation functions of every layer\n",
    "    :param input: Inputs given to the neural network\n",
    "    :return:\n",
    "        A_final, cache\n",
    "        A_final : Final output values of the DNN after forward propagation\n",
    "        cache : Dictionary\n",
    "                cache[\"Z\" + str(l)] : The weighed sum\n",
    "                cache[\"A\" + str(l)] : The value after passing the weighed sums through the activation function\n",
    "    \"\"\"\n",
    "    # Forward propagation process\n",
    "    cache = {\"A0\": input}\n",
    "    L = int(len(params.keys()) / 2)\n",
    "    for i in range(1, L + 1):\n",
    "        cache[\"Z\" + str(i)] = params[\"W\" + str(i)] @ cache[\"A\" + str(i - 1)] + params[\"b\" + str(i)]\n",
    "        act = activations[i - 1]()[\"function\"]\n",
    "        cache[\"A\" + str(i)] = act(cache[\"Z\" + str(i)])\n",
    "    A_final = cache[\"A\" + str(L)]\n",
    "    return A_final, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(A_pred, Y):\n",
    "    \"\"\"\n",
    "    The Chi Squared cost function employed for knowing the goodness of fit\n",
    "    :param A_pred: Predictions made\n",
    "    :param Y: Actual output from the dataset\n",
    "    :return:\n",
    "        Cost\n",
    "    \"\"\"\n",
    "    diff_vect = A_pred - Y\n",
    "    diff_vect = np.square(diff_vect)\n",
    "    cost_val = np.average(diff_vect)/2\n",
    "    return cost_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation gradient\n",
    "def backward_propagation_grads(cache, params, activations, Y):\n",
    "    \"\"\"\n",
    "    The backward propagation gradient generator\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Outputs\n",
    "    :return: return the gradients (changes to apply)\n",
    "        Dictionary\n",
    "            grads[\"dW\" + str(l)] : Gradient of weights of layer l\n",
    "            grads[\"db\" + str(l)] : Gradient of biases of layer l\n",
    "    \"\"\"\n",
    "    # Main variables\n",
    "    L = int(len(params.keys()) / 2)  # Number of layers in the neural network\n",
    "    m = cache[\"A0\"].shape[1]  # Number of training examples\n",
    "    # Calculate the multiplying factors\n",
    "    del_vals = {}\n",
    "    if activations[L - 1] != sigmoid:\n",
    "        activation_derivative = activations[L-1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m * cache[\"A\" + str(L)] * (1 - cache[\"A\" + str(L)]))) * \\\n",
    "                               activation_derivative(cache[\"Z\" + str(L)])\n",
    "    else:\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m))\n",
    "    for l in range(L - 1, 0, -1):  # Go backward from layer L-1 to 1 to calculate del_vals[\"del\" + l]\n",
    "        activation_derivative = activations[l-1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(l)] = (params[\"W\" + str(l + 1)].T @ del_vals[\"del\" + str(l + 1)]) * \\\n",
    "                                   activation_derivative(cache[\"Z\" + str(l)])\n",
    "    # Calculate final derivatives\n",
    "    grads = {}\n",
    "    # Final backward step\n",
    "    for l in range(L, 0, -1):\n",
    "        grads[\"dW\" + str(l)] = del_vals[\"del\" + str(l)] @ cache[\"A\" + str(l - 1)].T\n",
    "        grads[\"db\" + str(l)] = del_vals[\"del\" + str(l)] @ np.ones((m, 1))\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation step\n",
    "def back_propagation_deep(cache, params, activations, Y, learning_rate=0.01, reg_lambda = 1000):\n",
    "    \"\"\"\n",
    "    Perform one step of backward propagation\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Output\n",
    "    :param learning_rate: The learning_rate to use\n",
    "    :return: The new parameters of the neural network\n",
    "        Dictionary\n",
    "            params[\"W\" + str(l)] : The weights of layer l\n",
    "            params[\"b\" + str(l)] : The biases of layer l\n",
    "    \"\"\"\n",
    "    # Backward propagation step\n",
    "    L = int(len(params.keys()) / 2)\n",
    "    m = Y.shape[1]\n",
    "    grads = backward_propagation_grads(cache, params, activations, Y)\n",
    "    for l in range(L, 0, -1):  # Adjust gradients from layer L to 1\n",
    "        params[\"W\" + str(l)] = params[\"W\" + str(l)] * (1 - learning_rate * reg_lambda/m)\\\n",
    "                               - learning_rate * grads[\"dW\" + str(l)]\n",
    "        params[\"b\" + str(l)] = params[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
