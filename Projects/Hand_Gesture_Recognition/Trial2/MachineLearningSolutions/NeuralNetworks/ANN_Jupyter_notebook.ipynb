{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here, we'll implement our own neural network framework without using any modern distributions like tensorflow, scikit learn, etc. The purpose of this notebook is to explain the fundamentals of Neural Networks and to understand how they work.\n",
    "A Neural Network is basically multiple perceptrons in multiple layers. We already have implemented linear classification using perceptron [here](https://github.com/TheProjectsGuy/Blogger/blob/master/Projects/Hand_Gesture_Recognition/Trial2/MachineLearningSolutions/Perceptron/Perceptron_model_5_fingers_detection.ipynb).\n",
    "In this notebook, we'll solve the same problem using a neural network. \n",
    "\n",
    "# Problem statement\n",
    "We need to build an algorithm that recognizes the gesture shown by a hand. The gesture will be one of the below ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir_name=\"Data\", x_name=\"X.npy\",\n",
    "                 y_name=\"Y_one_hot_encoded.npy\", one_hot_index=0, shuffle_data=True, normalize_data = True, show_demo = False):\n",
    "    \"\"\"\n",
    "    Loads a dataset stored as a .npy file\n",
    "    :param dataset_dir_name: Name of the folder in which data is\n",
    "    :param x_name: Name of file which contains inputs (with .npy extension)\n",
    "    :param y_name: Name of file which contains outputs (with .npy extension)\n",
    "    :param one_hot_index: The index you're training for (pass -1 for passing raw output file), should be >= -1\n",
    "    :param shuffle_data: Shuffle the data after getting it from file\n",
    "    :return:\n",
    "        X, Y\n",
    "        X -> Inputs\n",
    "        Y -> Outputs\n",
    "        The output will be shuffled if shuffle_data is True, else it won't be shuffled\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    X = np.load(\"../{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                 f_name=x_name))\n",
    "    print(\"DATA DEBUG : Inputs shape is {in_shape}\".format(in_shape=X.shape))\n",
    "    if one_hot_index == -1:\n",
    "        # Parse the entire dataset as it is\n",
    "        Y = np.load(\"../{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                     f_name=y_name))\n",
    "    elif one_hot_index >= 0:\n",
    "        Y_one_hot = np.load(\"../{main_root}/{f_name}\".format(\n",
    "            main_root=dataset_dir_name, f_name=y_name\n",
    "        ))\n",
    "        Y = np.array(Y_one_hot[one_hot_index, :].reshape((1, -1)))\n",
    "    else:\n",
    "        raise IndexError(\"The index {ind_number} is an illegal index\".format(ind_number=one_hot_index))\n",
    "    print(\"DATA DEBUG : Output shape is {out_shape}\".format(out_shape=Y.shape))\n",
    "    if normalize_data:\n",
    "        X = X / 255\n",
    "    # Shuffle the dataset\n",
    "    def shuffle_dataset(X, Y):\n",
    "        buffer_data = np.row_stack((X, Y))\n",
    "        buffer_data = buffer_data.T\n",
    "        np.random.shuffle(buffer_data)\n",
    "        buffer_data = buffer_data.T\n",
    "        X = buffer_data[0:X.shape[0], :]\n",
    "        Y = buffer_data[X.shape[0]:X.shape[0] + Y.shape[0], :]\n",
    "        return X, Y\n",
    "\n",
    "    if shuffle_data:\n",
    "        X, Y = shuffle_dataset(X, Y)\n",
    "    if show_demo:\n",
    "        # Show a training example\n",
    "        plt.imshow(X[:,0].reshape((47,38)), cmap='gray')\n",
    "        plt.title(Y[:,0])\n",
    "        plt.show()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev_test(X, Y, sizes=(2500, 136, 136)):\n",
    "    \"\"\"\n",
    "    Split the dataset into train, dev and test sets\n",
    "    :param X: Inputs\n",
    "    :param Y: Outputs\n",
    "    :param sizes: Distribution tuple\n",
    "    :return: Dictionary\n",
    "        \"train\" : (X_train, Y_train)\n",
    "        \"dev\" : (X_dev, Y_dev)\n",
    "        \"test\" : (X_test, Y_test)\n",
    "    \"\"\"\n",
    "    # Split the dataset\n",
    "    X_train = X[:, 0:sizes[0]]\n",
    "    Y_train = Y[:, 0:sizes[0]]\n",
    "    X_dev = X[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    Y_dev = Y[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    X_test = X[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    Y_test = Y[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    print(\"DATA DEBUG : Train input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_train.shape, out_shape=Y_train.shape\n",
    "    ))\n",
    "    print(\"DATA DEBUG : Test input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_test.shape, out_shape=Y_test.shape\n",
    "    ))\n",
    "    print(\"DATA DEV : Dev input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_dev.shape, out_shape=Y_dev.shape\n",
    "    ))\n",
    "    rdict = {\n",
    "        \"train\": (X_train, Y_train),\n",
    "        \"dev\": (X_dev, Y_dev),\n",
    "        \"test\": (X_test, Y_test)\n",
    "    }\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_deep(input_size, layer_tup):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the DNN\n",
    "    :param input_size: Size of the input layer\n",
    "    :param layer_tup:  Tuple of layer sizes (hidden layers + output layer)\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"W + str(i)\" : Weight of layer i\n",
    "            \"b + str(i)\" : Biases of layer i\n",
    "    \"\"\"\n",
    "    # Initialize the neural network with parameters\n",
    "    layer_size = (input_size, *layer_tup)\n",
    "    print(\"DATA DEBUG : Initializing neural network with architecture {arc}\".format(arc=layer_size))\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_size)):\n",
    "        params[\"W\" + str(i)] = np.random.rand(layer_size[i], layer_size[i - 1]) * 10\n",
    "        params[\"b\" + str(i)] = np.zeros((layer_size[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_activations(activations):\n",
    "    \"\"\"\n",
    "    Parse activations into function and derivatives\n",
    "    :param activations: Function which returns a dictionary\n",
    "        \"function\" : Activation function\n",
    "        \"derivative\" : Derivative of function\n",
    "    :return:\n",
    "        activation_function, activation_function_derivatives\n",
    "    \"\"\"\n",
    "    activation_fncs = []\n",
    "    activation_fnc_der = []\n",
    "    for fnc in activations:\n",
    "        activation_fncs.append(fnc()[\"function\"])\n",
    "        activation_fnc_der.append(fnc()[\"derivative\"])\n",
    "    return activation_fncs, activation_fnc_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : sigmoid_function\n",
    "            \"derivative\" : sigmoid_derivative\n",
    "    \"\"\"\n",
    "    def sigmoid_function(x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid_function(x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid'(x)\n",
    "        \"\"\"\n",
    "        return sigmoid_function(x) * (1 - sigmoid_function(x))\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": sigmoid_function,\n",
    "        \"derivative\": sigmoid_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu():\n",
    "    \"\"\"\n",
    "    ReLU (Rectified linear unit) activation function\n",
    "    :param x:\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : relu_function\n",
    "            \"derivative\" : relu_derivative\n",
    "    \"\"\"\n",
    "    def relu_function(x):\n",
    "        \"\"\"\n",
    "        ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu(x)\n",
    "        \"\"\"\n",
    "        ret_x = x.copy()\n",
    "        ret_x[ret_x < 0] = 0\n",
    "        return ret_x\n",
    "\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu'(x)\n",
    "        \"\"\"\n",
    "        ret_d = x.copy()\n",
    "        ret_d[x <= 0] = 0\n",
    "        ret_d[x > 0] = 1\n",
    "        return ret_d\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": relu_function,\n",
    "        \"derivative\": relu_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate_deep(params, activations, input):\n",
    "    \"\"\"\n",
    "    Perform forward propagation of the neural network\n",
    "    :param params: Dictionary of weights and biases of the network\n",
    "            params[\"W\" + str(l)] : Weights of layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: Array of activation functions of every layer\n",
    "    :param input: Inputs given to the neural network\n",
    "    :return:\n",
    "        A_final, cache\n",
    "        A_final : Final output values of the DNN after forward propagation\n",
    "        cache : Dictionary\n",
    "                cache[\"Z\" + str(l)] : The weighed sum\n",
    "                cache[\"A\" + str(l)] : The value after passing the weighed sums through the activation function\n",
    "    \"\"\"\n",
    "    # Forward propagation process\n",
    "    cache = {\"A0\": input}\n",
    "    L = int(len(params.keys()) / 2)\n",
    "    for i in range(1, L + 1):\n",
    "        cache[\"Z\" + str(i)] = params[\"W\" + str(i)] @ cache[\"A\" + str(i - 1)] + params[\"b\" + str(i)]\n",
    "        act = activations[i - 1]()[\"function\"]\n",
    "        cache[\"A\" + str(i)] = act(cache[\"Z\" + str(i)])\n",
    "    A_final = cache[\"A\" + str(L)]\n",
    "    return A_final, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(A_pred, Y):\n",
    "    \"\"\"\n",
    "    The Chi Squared cost function employed for knowing the goodness of fit\n",
    "    :param A_pred: Predictions made\n",
    "    :param Y: Actual output from the dataset\n",
    "    :return:\n",
    "        Cost\n",
    "    \"\"\"\n",
    "    diff_vect = A_pred - Y\n",
    "    diff_vect = np.square(diff_vect)\n",
    "    cost_val = np.average(diff_vect)/2\n",
    "    return cost_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation gradient\n",
    "def backward_propagation_grads(cache, params, activations, Y):\n",
    "    \"\"\"\n",
    "    The backward propagation gradient generator\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Outputs\n",
    "    :return: return the gradients (changes to apply)\n",
    "        Dictionary\n",
    "            grads[\"dW\" + str(l)] : Gradient of weights of layer l\n",
    "            grads[\"db\" + str(l)] : Gradient of biases of layer l\n",
    "    \"\"\"\n",
    "    # Main variables\n",
    "    L = int(len(params.keys()) / 2)  # Number of layers in the neural network\n",
    "    m = cache[\"A0\"].shape[1]  # Number of training examples\n",
    "    # Calculate the multiplying factors\n",
    "    del_vals = {}\n",
    "    if activations[L - 1] != sigmoid:\n",
    "        activation_derivative = activations[L-1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m * cache[\"A\" + str(L)] * (1 - cache[\"A\" + str(L)]))) * \\\n",
    "                               activation_derivative(cache[\"Z\" + str(L)])\n",
    "    else:\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m))\n",
    "    for l in range(L - 1, 0, -1):  # Go backward from layer L-1 to 1 to calculate del_vals[\"del\" + l]\n",
    "        activation_derivative = activations[l-1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(l)] = (params[\"W\" + str(l + 1)].T @ del_vals[\"del\" + str(l + 1)]) * \\\n",
    "                                   activation_derivative(cache[\"Z\" + str(l)])\n",
    "    # Calculate final derivatives\n",
    "    grads = {}\n",
    "    # Final backward step\n",
    "    for l in range(L, 0, -1):\n",
    "        grads[\"dW\" + str(l)] = del_vals[\"del\" + str(l)] @ cache[\"A\" + str(l - 1)].T\n",
    "        grads[\"db\" + str(l)] = del_vals[\"del\" + str(l)] @ np.ones((m, 1))\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation step\n",
    "def back_propagation_deep(cache, params, activations, Y, learning_rate=0.01, reg_lambda = 1000):\n",
    "    \"\"\"\n",
    "    Perform one step of backward propagation\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Output\n",
    "    :param learning_rate: The learning_rate to use\n",
    "    :return: The new parameters of the neural network\n",
    "        Dictionary\n",
    "            params[\"W\" + str(l)] : The weights of layer l\n",
    "            params[\"b\" + str(l)] : The biases of layer l\n",
    "    \"\"\"\n",
    "    # Backward propagation step\n",
    "    L = int(len(params.keys()) / 2)\n",
    "    m = Y.shape[1]\n",
    "    grads = backward_propagation_grads(cache, params, activations, Y)\n",
    "    for l in range(L, 0, -1):  # Adjust gradients from layer L to 1\n",
    "        params[\"W\" + str(l)] = params[\"W\" + str(l)] * (1 - learning_rate * reg_lambda/m)\\\n",
    "                               - learning_rate * grads[\"dW\" + str(l)]\n",
    "        params[\"b\" + str(l)] = params[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make our DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "X, Y = load_dataset()\n",
    "datasets = split_train_dev_test(X, Y)\n",
    "X_train, Y_train = datasets[\"train\"]\n",
    "X_dev, Y_dev = datasets[\"dev\"]\n",
    "X_test, Y_test = datasets[\"test\"]\n",
    "# Load weights and activation functions\n",
    "architecture_nn, params = init_params_deep(X.shape[0], (50, 50, 1))\n",
    "activations = [relu, relu, sigmoid]\n",
    "# Training the network\n",
    "num_iter = 20\n",
    "debug_iter_num = 10\n",
    "cost_tracker = {\n",
    "    \"train_x\" : [],\n",
    "    \"train_cost\" : [],\n",
    "    \"eval_x\" : [],\n",
    "    \"eval_cost\" : []\n",
    "}\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "reg_param_lambda = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    # Forward propagate\n",
    "    A_pred, cache = forward_propagate_deep(params, activations, X_train)\n",
    "    # Note the cost\n",
    "    cost_iter = cost_function(A_pred, Y_train)\n",
    "    cost_tracker[\"train_x\"].append(i)\n",
    "    cost_tracker[\"train_cost\"].append(cost_iter)\n",
    "    if (i+1) % debug_iter_num == 0:\n",
    "        print(\"TRAIN DEBUG : Cost at iteration {it_num} is {cost}\".format(cost=cost_iter, it_num=i+1))\n",
    "        pred_test, _ = forward_propagate_deep(params, activations, X_test)\n",
    "        cost_test = cost_function(pred_test, Y_test)\n",
    "        cost_tracker[\"eval_x\"].append(i)\n",
    "        cost_tracker[\"eval_cost\"].append(cost_test)\n",
    "    # Back propagation\n",
    "    params = back_propagation_deep(cache, params, activations, Y_train, learning_rate)\n",
    "\n",
    "plt.plot(cost_tracker[\"train_x\"], cost_tracker[\"train_cost\"], 'b-',\n",
    "         cost_tracker[\"eval_x\"], cost_tracker[\"eval_cost\"], 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_test_set(test_x, test_y, params):\n",
    "    A_pred, _ = forward_propagate_deep(params, activations, test_x)\n",
    "    predictions = np.zeros_like(A_pred)\n",
    "    predictions[A_pred > 0.5] = 1\n",
    "    diff_vector = predictions - test_y\n",
    "    diff_vector = np.square(diff_vector)\n",
    "    mismatch_vector = diff_vector[diff_vector == 1].reshape((1, -1))\n",
    "    print(\"{err}% mismatch error\".format(err=diff_vector.shape[1]/mismatch_vector.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test_set(X_test, Y_test, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
