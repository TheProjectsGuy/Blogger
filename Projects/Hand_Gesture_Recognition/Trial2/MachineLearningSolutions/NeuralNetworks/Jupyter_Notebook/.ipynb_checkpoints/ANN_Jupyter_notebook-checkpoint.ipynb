{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A neural network, sometimes also referred as multi layer perceptron, is basically an algorithm that is used to learn more sophisticated features than what a perceptron can learn. It does this by using multiple layers of perceptrons. The image below explains a little about the architecture it follows\n",
    "<center> **Neural Network** </center>\n",
    "![Neural Network architecture](NN_demo_image.jpeg \"A Neural Network\")\n",
    "\n",
    "The inputs are given to the *input layer*, the hidden layers simply perform a weighed sum of the outputs of the previous layer, add it with a bias and then pass it through a function. Usually, the same activation function is used for every neuron in one layer. If you've gone through my perceptron notebook (link [here](https://github.com/TheProjectsGuy/Blogger/blob/master/Projects/Hand_Gesture_Recognition/Trial2/MachineLearningSolutions/Perceptron/Jupyter_Notebook/Perceptron_model_5_fingers_detection.ipynb)), you can perceive that every neuron is basically acting like a perceptron (hence the name _multi layer perceptron_). All the neurons are connected by a _weight_, this is used to perform a weighed sum (same as in the perceptron)\n",
    "\n",
    "Basically to summarize what happens in a neural network, \n",
    "- *Neurons of input layer*\n",
    "   - Simply take the inputs\n",
    "- *Neurons of hidden layers*\n",
    "   - Take values of the previous payer\n",
    "   - Perform a weighed sum\n",
    "   - Add a bias\n",
    "   - Pass the result through a function (known as the activation function of the layer)\n",
    "   - Store the output\n",
    "- *Neurons of output layer*\n",
    "   - Take the values of the previous layer\n",
    "   - Perform a weighed sum\n",
    "   - Add a bias\n",
    "   - Pass the result through a function\n",
    "   - Present the output. This is the _output of the neural network_.\n",
    "\n",
    "The above points are exactly what we do in forward propagation through the neural network. In order to learn the proper weights connecting the neurons from layer _l_ to _l + 1_, we perform something called as _Back Propagation_. In this, we apply the concept of gradient descent, chain rule of differentiation and a little matrice calculus to obtain the changes we need to make.\n",
    "\n",
    "In the [perceptron](https://github.com/TheProjectsGuy/Blogger/blob/master/Projects/Hand_Gesture_Recognition/Trial2/MachineLearningSolutions/Perceptron/Jupyter_Notebook/Perceptron_model_5_fingers_detection.ipynb) algorithm, we used the entire training set in order to take only one step, and then took several iterations of it. Here, we'll try something different. We'll divide the training set into many smaller training sets (known as batches), and then perform training on batches. This is known as _mini batch gradient descent_. We'll program that too. We'll not use any of the modern programming frameworks like [_tensorflow_](https://www.tensorflow.org/), [_scikit learn_](http://scikit-learn.org/stable/index.html), or anything. We'll make one from the basics to understand how everything works in a simple neural network. So let's get started\n",
    "\n",
    "## _Hand Gesture Recognition_\n",
    "Here are the different hand gestures we'll be working with. For this, we'll follow the same thing we did in the perceptron algorithm. We'll train a neural network to detect the five finger gesture.\n",
    "<center> **Gestures** </center>\n",
    "![Gestures](Hand_Gesture_Classes_5_Finger.jpg)\n",
    "Usually, there's an intuition in deciding the number of layers. We can attach them wth minor features but we're free to explore possibilities.\n",
    "\n",
    "Let's start by importing the needed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np                     # For optimised mathematical operations\n",
    "from matplotlib import pyplot as plt   # For plotting the results \n",
    "\n",
    "np.random.seed(2)       # Just so that we get the same random number every time we run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and shuffling data\n",
    "Now we're dealing with a supervised learning problem (labelled dataset needed). For this neural network, we have a dataset consisting of binary images (every pixel is either white or black). The images are actually 235 by 190, but they're shrinked to 47 by 38 size, so that we have smaller number of input featres to deal with (1786 input features). Fortunately, we have a **.npy** file for inputs and labelled outputs, so all we need to do is to load the binary data file.\n",
    "Usually, we are unaware of the source of data, so it's always a good idea to shuffle it. This is done so that we obtain a uniform distribution of data, this improves overall performance. The dataset we are using has about a 50-50 distribution (50 % images are images of 5 fingers), shuffling it will ensure uniform distribution.\n",
    "\n",
    "### Splitting data into buckets\n",
    "To train and evaluate the model, we usually split the data into three groupt (buckets), **train**, **dev** and **test** sets. Their purpose is stated below\n",
    "\n",
    "Bucket name | Purpose\n",
    "-----|------\n",
    "**Train** | Used to train a model\n",
    "**Dev** | Used to compare different models (also known as the cross validation set)\n",
    "**Test** | Used to perform the final test of the model\n",
    "\n",
    "While training, we split the training set into various parts (batches), let us implement a function for that as well.\n",
    "\n",
    "So first, *loading the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(dataset_dir_name=\"Data\", x_name=\"X.npy\",\n",
    "                 y_name=\"Y_one_hot_encoded.npy\", one_hot_index=0, shuffle_data=True, normalize_data=True):\n",
    "    \"\"\"\n",
    "    Loads a dataset stored as a .npy file\n",
    "    :param dataset_dir_name: Name of the folder in which data is\n",
    "    :param x_name: Name of file which contains inputs (with .npy extension)\n",
    "    :param y_name: Name of file which contains outputs (with .npy extension)\n",
    "    :param one_hot_index: The index you're training for (pass -1 for passing raw output file), should be >= -1\n",
    "    :param shuffle_data: Shuffle the data\n",
    "    :param normalize_data: Normalize the input data\n",
    "    :return:\n",
    "        X, Y\n",
    "        X -> Inputs\n",
    "        Y -> Outputs\n",
    "        The output will be shuffled if shuffle_data is True, else it won't be shuffled\n",
    "    \"\"\"\n",
    "    # Load the dataset from memory\n",
    "    X = np.load(\"{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                 f_name=x_name))\n",
    "    print(\"DATA DEBUG : Inputs shape is {in_shape}\".format(in_shape=X.shape))\n",
    "    if one_hot_index == -1:\n",
    "        # Parse the entire dataset as it is\n",
    "        Y = np.load(\"{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                     f_name=y_name))\n",
    "    elif one_hot_index >= 0:\n",
    "        # Get the particular one_hot_encoded row\n",
    "        Y_one_hot = np.load(\"{main_root}/{f_name}\".format(\n",
    "            main_root=dataset_dir_name, f_name=y_name\n",
    "        ))\n",
    "        Y = np.array(Y_one_hot[one_hot_index, :].reshape((1, -1)))\n",
    "    else:\n",
    "        raise IndexError(\"The index {ind_number} is an illegal index\".format(ind_number=one_hot_index))\n",
    "    print(\"DATA DEBUG : Output shape is {out_shape}\".format(out_shape=Y.shape))\n",
    "    if normalize_data:\n",
    "        X = X / 255\n",
    "    if shuffle_data:\n",
    "        X, Y = shuffle_dataset(X, Y)\n",
    "    Y_true_p = Y.nonzero()[1].reshape(1, -1).shape[1] / Y.shape[1]\n",
    "    print(\"DATA DEBUG : {tp}% data is true\".format(tp=Y_true_p * 100))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "def shuffle_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    Shuffles the dataset (shuffle columns) and returns it\n",
    "    :param X : Inputs\n",
    "    :param Y : Outputs\n",
    "    :return:\n",
    "        tuple(X, Y)\n",
    "        X, Y with their columns shuffled\n",
    "    \"\"\"\n",
    "    buffer_data = np.row_stack((X, Y))                        # Unify the inputs and outputs of data\n",
    "    buffer_data = buffer_data.T                               # columns -> rows\n",
    "    np.random.shuffle(buffer_data)                            # Performs row shuffling\n",
    "    buffer_data = buffer_data.T                               # rows -> columns\n",
    "    X = buffer_data[0:X.shape[0], :]                          # Retrieve X\n",
    "    Y = buffer_data[X.shape[0]:X.shape[0] + Y.shape[0], :]    # Retrieve Y\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to split the dataset into **train**, **dev** and **test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "def split_train_dev_test(X, Y, sizes=(2500, 136, 136)):\n",
    "    \"\"\"\n",
    "    Split the dataset into train, dev and test sets\n",
    "    :param X: Inputs\n",
    "    :param Y: Outputs\n",
    "    :param sizes: Distribution tuple\n",
    "    :return: Dictionary\n",
    "        \"train\" : (X_train, Y_train)\n",
    "        \"dev\" : (X_dev, Y_dev)\n",
    "        \"test\" : (X_test, Y_test)\n",
    "    \"\"\"\n",
    "    # Split the dataset\n",
    "    # Train sets\n",
    "    X_train = X[:, 0:sizes[0]]\n",
    "    Y_train = Y[:, 0:sizes[0]]\n",
    "    # Dev set\n",
    "    X_dev = X[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    Y_dev = Y[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    # Test set\n",
    "    X_test = X[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    Y_test = Y[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    print(\"DATA DEBUG : Train input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_train.shape, out_shape=Y_train.shape\n",
    "    ))\n",
    "    print(\"DATA DEBUG : Test input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_test.shape, out_shape=Y_test.shape\n",
    "    ))\n",
    "    print(\"DATA DEV : Dev input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_dev.shape, out_shape=Y_dev.shape\n",
    "    ))\n",
    "    rdict = {\n",
    "        \"train\": (X_train, Y_train),\n",
    "        \"dev\": (X_dev, Y_dev),\n",
    "        \"test\": (X_test, Y_test)\n",
    "    }\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to divide a dataset into batches of particular size (mini batches of *mini_batch_size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the set into mini_batches\n",
    "def divide_into_mini_batches(X, Y, mini_batch_size, debugger_output=False):\n",
    "    \"\"\"\n",
    "    Divides the passes set into mini batches\n",
    "    :param X : Inputs\n",
    "    :param Y : Outputs\n",
    "    :param mini_batch_size : The size of the mini batches required (if it's not an exact fit, then \n",
    "                             the last mini batch won't be of this size)\n",
    "    :param debugger_output : if True then the function will print the status of batch_splitting \n",
    "    :return: Tuple\n",
    "            (X_mini_batches, Y_mini_batches)\n",
    "            X_mini_batches : Containing arrays of input batches\n",
    "            Y_mini_batches : Containing arrays of output batches\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    n_batches = m // mini_batch_size\n",
    "    if debugger_output:\n",
    "        print(\"DATA DEBUG : Dividing {num} examples into batches of size {batch_s}. {n_b} full batches\".format(\n",
    "        num=m, batch_s=mini_batch_size, n_b = n_batches\n",
    "    ))\n",
    "    X_mini_batches = []\n",
    "    Y_mini_batches = []\n",
    "    for i in range(n_batches):\n",
    "        X_mini_batches.append(X[:, i * mini_batch_size : (i + 1) * mini_batch_size])\n",
    "        Y_mini_batches.append(Y[:, i * mini_batch_size : (i + 1) * mini_batch_size])\n",
    "    if m % mini_batch_size != 0:\n",
    "        X_mini_batches.append(X[:, n_batches * mini_batch_size : ])\n",
    "        Y_mini_batches.append(Y[:, n_batches * mini_batch_size : ])\n",
    "    return (X_mini_batches, Y_mini_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the neural network\n",
    "### Declaration\n",
    "Here comes the part where we plan out the architecture of the neural network (also known as the _model architecture_). Let us decide the following before moving further :\n",
    "- Number of hidden layers\n",
    "- Number of neurons in every layer\n",
    "- Activations of every layer\n",
    "\n",
    "The layer, number of neurons, activations and description are shown in the table below\n",
    "\n",
    "| Layer number | Number of neurons | Activation | Description |\n",
    "| ----|----|----|----| \n",
    "| 0 | 1786 | ```None``` | Input layer\n",
    "| 1 | 100 | ```tanh``` | First hidden layer\n",
    "| 2 | 50 | ```tanh``` | Second hidden layer\n",
    "| 3 | 50 | ```relu``` | Third hidden layer\n",
    "| 4 | 50 | ```relu``` | Fourth hidden layer\n",
    "| 5 | 5 | ```relu``` | Fifth hidden layer\n",
    "| 6 | 1 | ```sigmoid``` | Output layer\n",
    "\n",
    "Things to note from above are that the input layer is not counted in the number of layers and it doesn't have any activation function (it simply receives and gives the input to the neural network), the output layer (in case of classification problems) has a sigmoid activation function.\n",
    "\n",
    "### Initialization\n",
    "As discussed earlier, every two consecutive layers have a set of connections, which we'll call **weights**, additionally, every neuron of layers other than input have a **bias** (which is added to the weighed sum). These are called _parameters_ in our code. \n",
    "\n",
    "For the purpose of this notebook, we'll use the following notation\n",
    "\n",
    "- $N^{l}$ means the number of neurons in layer *l*. **L** is the number of layers so $l\\in \\left [ 0,L \\right ]$\n",
    "- $W^{l}$ means the weights connecting layer *l-1* and *l*. Thus it's a matrix of dimension $N^{l}$ rows and $N^{l-1}$ columns\n",
    "    - Further, $W_{i}^{l}$ is a row vector of the connection weights for the *i*<sup>th</sup> neuron of *l*<sup>th</sup> layer. Hence, it's shape (dimension) is 1 row by $N^{l-1}$ columns\n",
    "- $b^{l}$ means the biases for the neurons of the layer *l*. It's shape is *l* rows and 1 column\n",
    "\n",
    "When we initialize the weights, we **don't** set them to 0. This is because we don't want all of them to get updated in the same manner. This is called _breaking the symmetry_ of the neural network. We therefore initialize them to random numbers. It makes sence for the layers having big number of inputs to have smaller weights at initialization, this keeps the results away from extreme. We don't want high values for the weighed sums because if you observe the graphs of _tanh_ and _sigmoid_ functions (below) you can see that their values barely change at extremes (their derivative is 0), this will badly affect the gradient descent algorithm (as we'll see later).\n",
    "<img src=\"sigmoid_tanh_function_graph.png\" alt=\"Activation Functions\" height=200 width=300>\n",
    "\n",
    "There are multiple initialization techniques used, we'll use the one shown below\n",
    "```python\n",
    "params[\"W\" + str(i)] = np.random.randn(layer_size[i], layer_size[i - 1]) * 2 / np.sqrt(layer_size[i - 1])\n",
    "```\n",
    "This is also called the _He Initialization technique_. Let's initialize the parameters according to the above rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters of the neural network\n",
    "def init_params_deep(input_size, layer_tup):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the DNN\n",
    "    :param input_size: Size of the input layer\n",
    "    :param layer_tup:  Tuple of layer sizes (hidden layers + output layer)\n",
    "    :return:\n",
    "        layers_info, parameters\n",
    "        :return layer_size\n",
    "            The size architecture of the neural network. Sizes of every layer including input layer\n",
    "        :return params\n",
    "        Dictionary\n",
    "            \"W + str(i)\" : Weight of layer i\n",
    "            \"b + str(i)\" : Biases of layer i\n",
    "    \"\"\"\n",
    "    # Initialize the neural network with parameters\n",
    "    layer_size = (input_size, *layer_tup)\n",
    "    print(\"DATA DEBUG : Initializing neural network with architecture {arc}\".format(arc=layer_size))\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_size)):\n",
    "        params[\"W\" + str(i)] = np.random.randn(layer_size[i], layer_size[i - 1]) * 2 / np.sqrt(layer_size[i - 1])\n",
    "        params[\"b\" + str(i)] = np.zeros((layer_size[i], 1))\n",
    "    return layer_size, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "An activation function is one of the crucial reasons why a neural network even works. Say we used no activation functions (just performed the weighed sum, added bias and then passed it just as it is), there would be a problem. We'll just be performing a weighed sum over weighed sum over weighed sum, which ultimately boils down to a *single weighed sum* which defies the reason why we even use a neural network, we need it to explore way more features, sophisticated features. \n",
    "\n",
    "Now that we know that using an activation function for every layer is important, the question is what activation function should we use for different layers ?<br>\n",
    "- Well, it depends. If you're doing a classification problem (like we're doing here), it's suggested that the output layer has the ```sigmoid``` function. For regression based problem (guessing a range, eg: housing price prediction), we usually use the ```relu``` function.\n",
    "- For hidden layers, you're free to explore different possibilities\n",
    "\n",
    "We need to implement different functions and their derivatives as well (we'll need the derivatives for gradient descent algorithm). It's a good idea to follow a particular pattern in declaration (so that experimenting is easier). <br>\n",
    "We'll start by making an _activation parser_, which splits the activation function and it's derivative. We do this so that it's easier to handle during forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split activation function into function and it's derivative\n",
    "def parse_activations(activations):\n",
    "    \"\"\"\n",
    "    Parse activations into function and derivatives\n",
    "    :param activations: Function which returns a dictionary\n",
    "        \"function\" : Activation function\n",
    "        \"derivative\" : Derivative of function\n",
    "    :return:\n",
    "        activation_function, activation_function_derivatives\n",
    "    \"\"\"\n",
    "    activation_fncs = []\n",
    "    activation_fnc_der = []\n",
    "    for fnc in activations:\n",
    "        activation_fncs.append(fnc()[\"function\"])\n",
    "        activation_fnc_der.append(fnc()[\"derivative\"])\n",
    "    return activation_fncs, activation_fnc_der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function\n",
    "This is probably the most common activation function used in neural networks. It's graph is shown below\n",
    "<img src=\"sigmoid_function.png\" width=500>\n",
    "It's mathematical formula is \n",
    "$$ sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "It's derivative is \n",
    "$$ \\frac{\\partial sigmoid(x)}{\\partial x} = \\frac{e^{-x}}{(1+e^{-x})^{2}} = sigmoid(x) \\times (1-sigmoid(x)) $$\n",
    "Let's impliment this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Activation function\n",
    "def sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : sigmoid_function\n",
    "            \"derivative\" : sigmoid_derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def sigmoid_function(x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid_function(x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid'(x)\n",
    "        \"\"\"\n",
    "        return sigmoid_function(x) * (1 - sigmoid_function(x))\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": sigmoid_function,\n",
    "        \"derivative\": sigmoid_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation function\n",
    "This is another commonly used activation function for neural networks. It's graph is shown below\n",
    "<img src=\"relu_function.png\" width=400>\n",
    "It's mathematically defined as \n",
    "$$ReLU(x) = \\left\\{\\begin{matrix}\n",
    "0 & x \\leq 0\\\\\n",
    "x & x > 0 \n",
    "\\end{matrix}\\right.$$\n",
    "It's derivative is \n",
    "$$\\frac{\\partial ReLU(x)}{\\partial x} = \\left\\{\\begin{matrix}\n",
    "0 & x \\leq 0\\\\\n",
    "1 & x > 0 \n",
    "\\end{matrix}\\right.$$\n",
    "Let's implement this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu():\n",
    "    \"\"\"\n",
    "    ReLU (Rectified linear unit) activation function\n",
    "    :param x:\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : relu_function\n",
    "            \"derivative\" : relu_derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def relu_function(x):\n",
    "        \"\"\"\n",
    "        ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu(x)\n",
    "        \"\"\"\n",
    "        ret_x = x.copy()\n",
    "        ret_x[ret_x < 0] = 0\n",
    "        return ret_x\n",
    "\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu'(x)\n",
    "        \"\"\"\n",
    "        ret_d = x.copy()\n",
    "        ret_d[x <= 0] = 0\n",
    "        ret_d[x > 0] = 1\n",
    "        return ret_d\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": relu_function,\n",
    "        \"derivative\": relu_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic tan activation function\n",
    "This is a common function when we want the effects of sigmoid but a little broader than sigmoid (this has values between -1 and 1). It's graph is shown below\n",
    "<img src=\"tanh_function.png\" width=300>\n",
    "It's mathematically defined as \n",
    "$$tanh(x)=\\frac{sinh(x)}{cosh(x)}=\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "Remember that $sinh(x)=\\frac{e^{x}-e^{-x}}{2}$ and $cosh(x)=\\frac{e^{x}+e^{-x}}{2}$\n",
    "\n",
    "The derivative is given by \n",
    "$$\\frac{\\partial tanh(x)}{\\partial x} = \\frac{4 e^{2x}}{(e^{2x}+1)^{2}} = \\left ( \\frac{2}{e^{x} + e^{-x}} \\right )^{2}= \\frac{1}{cosh(x)}$$\n",
    "Let's implement this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh Activation function\n",
    "def tanh():\n",
    "    \"\"\"\n",
    "    Tan Hyperbolic activation function\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : Tan hyperbolic function\n",
    "            \"derivative\" : Derivative of tan hyperbolic function\n",
    "    \"\"\"\n",
    "\n",
    "    def tanh_function(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(x):\n",
    "        return np.square(1 / (np.cosh(x)))\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": tanh_function,\n",
    "        \"derivative\": tanh_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "As discussed in the beginning, forward propagation is the process of getting the predictions from the neual network. We will perform the following from layer 1 to L.\n",
    "1. Take outputs of previous layer\n",
    "2. Perform the weighed sum with the weights matrice\n",
    "3. Add bias\n",
    "4. Pass the result through the activation function of the layer\n",
    "\n",
    "The output is set to the output of the last step.<br>\n",
    "A simple way of doing steps 1 through 3 is to do a matrice multiplication of weights and the outputs of previous layer. Mathematical steps are as follows\n",
    "\n",
    "$$Z^{l} = W^{l} \\times A^{l-1} + b^{l}$$\n",
    "$$A^{l} = G^{l}(Z^{l})$$\n",
    "\n",
    "where $A^{l}$ is the output of layer $l$, $Z^{l}$ is the weighed sum performed for the layer $l$ and $G^{l}(...)$ is the activation function for the layer $l$. Remember that $A^{0}$ is nothing but the input given to the neural network. Also, to perform matrice multiplication, we can use the ```@``` operator in python or use the function ```np.matmul(a,b)```. I've used the latter for clarification. Let's implement this through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation step\n",
    "def forward_propagate_deep(params, activations, input):\n",
    "    \"\"\"\n",
    "    Perform forward propagation of the neural network\n",
    "    :param params: Dictionary of weights and biases of the network\n",
    "            params[\"W\" + str(l)] : Weights of layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: Array of activation functions of every layer\n",
    "    :param input: Inputs given to the neural network\n",
    "    :return:\n",
    "        A_final, cache\n",
    "        A_final : Final output values of the DNN after forward propagation\n",
    "        cache : Dictionary\n",
    "                cache[\"Z\" + str(l)] : The weighed sum\n",
    "                cache[\"A\" + str(l)] : The value after passing the weighed sums through the activation function\n",
    "    \"\"\"\n",
    "    # Forward propagation process\n",
    "    cache = {\"A0\": input}                # This stores all the acivations and weighed sums of the DNN\n",
    "    L = int(len(params.keys()) / 2)      # Number of layers\n",
    "    for i in range(1, L + 1):\n",
    "        cache[\"Z\" + str(i)] = np.matmul(params[\"W\" + str(i)], cache[\"A\" + str(i - 1)]) + params[\"b\" + str(i)]    # Weighed sum performed\n",
    "        act = activations[i - 1]()[\"function\"]              # Activation function of layer i\n",
    "        cache[\"A\" + str(i)] = act(cache[\"Z\" + str(i)])      # Output of layer i\n",
    "    A_final = cache[\"A\" + str(L)]     # Output of the neural network\n",
    "    return A_final, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "Let's know about the back propagation algorithm. This is the actual _magic_ in learning of neural networks. In this, we basically define a _loss metric_ (a cost function), and minimize all the parameters of the neural network with reference to that function. The algorithm that does this is called **Gradient Descent** . <br>\n",
    "Let's see how this algorithm works by assuming only one parameter to adjust (*w*)<br>\n",
    "- We first define a cost function (which is a measure of penalty on *w* for missing it's optimal target), *J(w)*. It is obvious that we want minimum cost for the model, so we need to adjust *w* in such a way that we get minimum cost.\n",
    "- If we differentiate *J* with respect to *w*, we get the slope of a line (tangent). If we move ahead on the tangent, we'll reach the peak, so we need to move backwards. That is, if the slope is +ve, then we need to move towards left, if the slope is -ve, then we need to move towards right.\n",
    "- We'll take a tiny step in the needed direction and perform the same process iteratively until convergence of parameters (till parameters don't change significantly) or for some number of iterations.\n",
    "![Gradient Descent](gradient-descent.png \"Gradient Descent\")\n",
    "This was a brief information about gradient descent, for more you can check [this](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) out.<br>\n",
    "\n",
    "## Mathematical principle of back propagation\n",
    "\n",
    "For one or two parameters, it's easy to visualize what's happening. But, we have a lot more (all the weights and biases are 'parameters' to learn), so let's check the maths behind it<br>\n",
    "We know <br>\n",
    "<center>\n",
    "$W^{l} := W^{l} - \\alpha \\times \\frac{\\partial J}{\\partial W^{l}}$ and $b^{l} := b^{l} - \\alpha \\times \\frac{\\partial J}{\\partial b^{l}}$\n",
    "</center>\n",
    "Here, $\\alpha$ is called the learning rate, it specifies how aggresively we take steps towards the minima.\n",
    "The := implies to calculate the value to the right and assign it to the left variable<br>\n",
    "We define the cost funcion as given below\n",
    "$$J(\\hat{Y}, Y) = - \\frac{1}{m} \\left [\\sum_{j = 1}^{m} Y_{j} \\times ln(\\hat{Y}_{j}) + (1-Y_{j}) \\times ln(1-\\hat{Y}_{j}) \\right ]$$\n",
    "$Y_{j}$ is the label of *j*<sup>th</sup> training example (which is either 0 or 1), and $\\hat{Y}_{j}$ (or $A_{j}$ or $A^{L}_{j}$ is the prediction of our neural network for the *j*<sup>th</sup> training example. It can easily be shown that the above function has very high value when the prediction is the binary inverse of actual value. We take the average over all the training examples for the final cost. We can also write the cost as $J(A^{L})$.<br>\n",
    "**Let's see what happens to the weights of the last layer : $W^{L}$ and $b^{L}$**\n",
    "<center>\n",
    "$W^{L} := W^{L} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial W^{L}}$ and $b^{L} := b^{L} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial b^{L}}$\n",
    "</center>\n",
    "We only need to calculate the value of $\\frac{\\partial J(A^{L})}{\\partial W^{L}}$ to get the change.<br>\n",
    "We can write $\\frac{\\partial J(A^{L})}{\\partial W^{L}}$ as $\\frac{\\partial J(A^{L})}{\\partial A^{L}} \\times \\frac{\\partial A^{L}}{\\partial W^{L}}$ (This is known as chain rule). We can do the following calculations : <br>\n",
    "$\\frac{\\partial J(A^{L})}{\\partial A^{L}} = J'(A^{L})=\\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}$ and $\\frac{\\partial A^{L}}{\\partial W^{L}} = \\frac{\\partial [G^{L}(W^{L}\\times A^{L-1} + b^{L})]}{\\partial W^{L}} = G'^{L}(W^{L}\\times A^{L-1} + b^{L}) \\ast \\left ( A^{L-1} \\right )^{T} = G'^{L}(Z^{L})\\ast \\left ( A^{L-1} \\right )^{T}$. By the multiplying both of these, we get the following \n",
    "$$\\frac{\\partial J(A^{L})}{\\partial W^{L}} = \\left [ \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}\\cdot G'^{L}(Z^{L}) \\right ]\\times \\left(A^{L-1} \\right )^{T}$$\n",
    "Here, $(A^{L-1})^{T}$ is the transpose of $A^{L-1}$<br>\n",
    "Similarly, if we perform the same operations for the bias, we get\n",
    "$$\\frac{\\partial J(A^{L})}{\\partial b^{L}} = \\left [ \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}\\cdot G'^{L}(Z^{L}) \\right ]\\times \\Upsilon_{m,1}$$\n",
    "The $\\Upsilon_{m,1}$ indicates a column vector of shape *m* rows (number of training examples) and 1 column consisting of ones only.<br><br>\n",
    "**Let's see what happens to the weights of the second last layer : $W^{L-1}$ and $b^{L-1}$**\n",
    "<center>\n",
    "$W^{L-1} := W^{L-1} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial W^{L-1}}$ and $b^{L-1} := b^{L-1} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial b^{L-1}}$\n",
    "</center>\n",
    "We can write $\\frac{\\partial J(A^{L})}{\\partial W^{L-1}}$ as $\\frac{\\partial J(A^{L})}{\\partial A^{L}} \\times \\frac{\\partial A^{L}}{\\partial W^{L-1}}$, applying the chain rule again gives us $\\frac{\\partial J(A^{L})}{\\partial A^{L}} \\times \\frac{\\partial A^{L}}{\\partial A^{L-1}} \\times \\frac{\\partial A^{L-1}}{\\partial W^{L-1}}$. From this, we know that $\\frac{\\partial J(A^{L})}{\\partial A^{L}} = \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}$. We only need to find $\\frac{\\partial A^{L}}{\\partial A^{L-1}}$ and $\\frac{\\partial A^{L-1}}{\\partial W^{L-1}}$.<br>\n",
    "Here, we can approach a few standard rules derived below (for any *k*),\n",
    "$$\\frac{\\partial A^{k}}{\\partial A^{k-1}} = \\frac{\\partial G^{k}(Z^{k})}{\\partial A^{k-1}} = G'^{k}(Z^{k})\\cdot \\frac{\\partial (W^{k} \\times A^{k-1} + b^{k})}{\\partial A^{k-1}} = G'^{k}(Z^{k})\\cdot W^{k}$$\n",
    "and\n",
    "$$\\frac{\\partial A^{k}}{\\partial W^{k}} = \\frac{\\partial G^{k}(W^{k} \\times A^{k-1} + b)}{\\partial W^{k}} = \\frac{\\partial G^{k}(Z^{k})}{\\partial W^{k}} = G'^{k}(Z^{k})\\cdot \\frac{\\partial (W^{k} \\times A^{k-1} + b)}{\\partial W^{k}} = G'^{k}(Z^{k})\\cdot A^{k-1}$$\n",
    "This gives us following results (putting k = L in the first derived equation and k = L - 1 in the second derived equation)\n",
    "$$\\frac{\\partial J(A^{L})}{\\partial W^{L-1}} = \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(A^{L-1} \\right )^{T}$$\n",
    "$$\\frac{\\partial J(A^{L})}{\\partial b^{L-1}} = \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(\\Upsilon_{m,1} \\right )$$\n",
    "Which implies\n",
    "$$W^{L-1} = W^{L-1} - \\alpha \\cdot \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(A^{L-1} \\right )^{T}$$\n",
    "$$b^{L-1} = b^{L-1} - \\alpha \\cdot \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(\\Upsilon_{m,1} \\right )$$\n",
    "This is already becoming too big to write, so let's derive the rest in a recursive manner.\n",
    "Let us assume that every layer is collecting some cost multipliers (things in the square brackets) that we'll denote by $\\delta ^{l}$ ($\\delta$ stands for *del*) (for *l*<sup>th</sup> layer).\n",
    "We'll define the following : \n",
    "$$\\delta ^{L} = \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})$$\n",
    "$$\\delta ^{L-1} = \\left[\\left(W^{L} \\right )^{T} \\times \\delta ^{L} \\right ] \\cdot G'^{L-1}(Z^{L-1})$$\n",
    "This gives the general result (for *l* being one of the _hidden_ layers)\n",
    "$$\\delta ^{l} = \\left[\\left(W^{l+1} \\right )^{T} \\times \\delta ^{l+1} \\right ] \\cdot G'^{l}(Z^{l})$$\n",
    "The updation of parameters of the *l*<sup>th</sup> layer is done by the following formula :\n",
    "$$W^{l} := W^{l} - \\alpha \\left[\\delta ^{l} \\times \\left(A^{l-1} \\right )^{T} \\right ]$$\n",
    "$$b^{l} := b^{l} - \\alpha \\left[\\delta ^{l} \\times \\Upsilon_{m,1} \\right ]$$\n",
    "If you just focus on the above five formulas, that's the main concept. We use _regularization_ to avoid the weights from gaining big values, and thus avoiding overfitting (it reduces variance). For that, we simply add the square of every parameter in the neural network, multiply by a value ($\\lambda$) and divide the entire result by $2m$. Thus, on differentiation, we get the $\\lambda \\frac{param}{m}$ term in the square brackets as well, so we multiply every parameter by $(1 - \\frac{\\alpha \\lambda}{m})$ in the training loop.\n",
    "This gives us the following results\n",
    "$$W^{l} := W^{l} \\left (1 - \\frac{\\alpha \\lambda}{m}\\right ) - \\alpha \\left[\\delta ^{l} \\times \\left(A^{l-1} \\right )^{T} \\right ]$$\n",
    "$$b^{l} := b^{l} \\left (1 - \\frac{\\alpha \\lambda}{m}\\right ) - \\alpha \\left[\\delta ^{l} \\times \\Upsilon_{m,1} \\right ]$$\n",
    "So let's first write code to get the $\\delta$ values for the entire network and then the change in weights $dW^{l}$ and biases $db^{l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation gradient calculation\n",
    "def backward_propagation_grads(cache, params, activations, Y):\n",
    "    \"\"\"\n",
    "    The backward propagation gradient generator\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Outputs\n",
    "    :return: return the gradients (changes to apply)\n",
    "        Dictionary\n",
    "            grads[\"dW\" + str(l)] : Gradient of weights of layer l\n",
    "            grads[\"db\" + str(l)] : Gradient of biases of layer l\n",
    "    \"\"\"\n",
    "    # Main variables\n",
    "    L = int(len(params.keys()) / 2)  # Number of layers in the neural network\n",
    "    m = cache[\"A0\"].shape[1]  # Number of training examples\n",
    "    # Calculate the multiplying factors\n",
    "    del_vals = {}             # The del values\n",
    "    if activations[L - 1] != sigmoid:  # To simplify calculations if the last layer has sigmoid activation\n",
    "        activation_derivative = activations[L - 1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m * cache[\"A\" + str(L)] * (1 - cache[\"A\" + str(L)]))) * \\\n",
    "                                   activation_derivative(cache[\"Z\" + str(L)])\n",
    "    else:\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m))\n",
    "    for l in range(L - 1, 0, -1):  # Go backward from layer L-1 to 1 to calculate del_vals[\"del\" + l]\n",
    "        activation_derivative = activations[l - 1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(l)] = np.matmul(params[\"W\" + str(l + 1)].T, del_vals[\"del\" + str(l + 1)]) * \\\n",
    "                                   activation_derivative(cache[\"Z\" + str(l)])\n",
    "    # Calculate final derivatives\n",
    "    grads = {}\n",
    "    # Final backward step\n",
    "    for l in range(L, 0, -1):\n",
    "        grads[\"dW\" + str(l)] = np.matmul(del_vals[\"del\" + str(l)], cache[\"A\" + str(l - 1)].T)  # dW_l\n",
    "        grads[\"db\" + str(l)] = np.matmul(del_vals[\"del\" + str(l)], np.ones((m, 1)))            # db_l\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the final back propagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation step\n",
    "def back_propagation_deep(cache, params, activations, Y, learning_rate=0.01, reg_lambda=2):\n",
    "    \"\"\"\n",
    "    Perform one step of backward propagation\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Output\n",
    "    :param learning_rate: The learning_rate to use\n",
    "    :return: The new parameters of the neural network\n",
    "        Dictionary\n",
    "            params[\"W\" + str(l)] : The weights of layer l\n",
    "            params[\"b\" + str(l)] : The biases of layer l\n",
    "    \"\"\"\n",
    "    # Backward propagation step\n",
    "    L = int(len(params.keys()) / 2)\n",
    "    m = Y.shape[1]\n",
    "    grads = backward_propagation_grads(cache, params, activations, Y)\n",
    "    for l in range(L, 0, -1):  # Adjust gradients from layer L to 1\n",
    "        params[\"W\" + str(l)] = params[\"W\" + str(l)] * (1 - learning_rate * reg_lambda / m) \\\n",
    "                               - learning_rate * grads[\"dW\" + str(l)]\n",
    "        params[\"b\" + str(l)] = params[\"b\" + str(l)] * (1 - learning_rate * reg_lambda / m) \\\n",
    "                               - learning_rate * grads[\"db\" + str(l)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "Now let us define the performance metric function. I'm using the Chi squared cost function (it just finds the distance between the predictions and actual outputs in the *m* dimensional euclidian space, squares it, divides by 2 and then takes average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost_function(A_pred, Y):\n",
    "    \"\"\"\n",
    "    The Chi Squared cost function employed for knowing the goodness of fit\n",
    "    :param A_pred: Predictions made\n",
    "    :param Y: Actual output from the dataset\n",
    "    :return:\n",
    "        Cost\n",
    "    \"\"\"\n",
    "    diff_vect = A_pred - Y\n",
    "    diff_vect = np.square(diff_vect)\n",
    "    cost_val = np.average(diff_vect) / 2\n",
    "    return cost_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "We need to check how the neural network is performing on test data (some data that it has never seen while training).\n",
    "Let up implement this procedure as well with the help of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on the test set\n",
    "def error_test_set(test_x, test_y, params, activations, threshold=0.5,\n",
    "                   show_mismatch_images=True):\n",
    "    \"\"\"\n",
    "    To test the performance of the neural network\n",
    "    :param test_x: Inputs\n",
    "    :param test_y: Desired outputs\n",
    "    :param params: Parameters of the neural network\n",
    "    :param activations: Activation functions\n",
    "    :param threshold: Threshold value wanted\n",
    "    :param show_mismatch_images: If you want to view mismatch images\n",
    "    :return:\n",
    "    None\n",
    "    \"\"\"\n",
    "    predictions, _ = forward_propagate_deep(params, activations, test_x)\n",
    "    pred_test = np.zeros_like(predictions)\n",
    "    pred_test[predictions > threshold] = 1\n",
    "    difference_vector = pred_test - test_y\n",
    "    difference_vector = np.square(difference_vector)\n",
    "    diff_indices = difference_vector.nonzero()[1]\n",
    "    print(\"TEST DEBUG : {num_mismatch} mismatches found at indices {ind}\".format(\n",
    "        num_mismatch=len(diff_indices), ind=diff_indices\n",
    "    ))\n",
    "    if show_mismatch_images:\n",
    "        for ind in diff_indices:\n",
    "            print(\"TEST DEBUG : Testing image at index {i}\".format(i=ind))\n",
    "            img = test_x[:, ind].reshape((47, 38))\n",
    "            cv.imshow(\"Index {i}, Y = {y}, A = {a}\".format(\n",
    "                i=ind, y=test_y[:, ind], a=predictions[:, ind]\n",
    "            ), img)\n",
    "            while True:\n",
    "                key = cv.waitKey(0) & 0xff\n",
    "                if key == 27:\n",
    "                    exit(0)\n",
    "                elif key == ord('n'):\n",
    "                    cv.destroyWindow(\"Index {i}, Y = {y}, A = {a}\".format(\n",
    "                        i=ind, y=test_y[:, ind], a=predictions[:, ind]))\n",
    "                    break\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA DEBUG : Inputs shape is (1786, 921)\n",
      "DATA DEBUG : Output shape is (1, 921)\n",
      "DATA DEBUG : 50.0542888165038% data is true\n",
      "DATA DEBUG : Train input shape (1786, 900), output shape (1, 900)\n",
      "DATA DEBUG : Test input shape (1786, 21), output shape (1, 21)\n",
      "DATA DEV : Dev input shape (1786, 0), output shape (1, 0)\n",
      "DATA DEBUG : Initializing neural network with architecture (1786, 100, 50, 50, 50, 5, 1)\n",
      "TRAIN DEBUG : 240000 training iterations required, iteration limit is 13901\n",
      "TRAIN DEBUG : Cost at iteration 0 is 0.14721381691879284\t Test cost is 0.18141885860279666\n",
      "TRAIN DEBUG : Cost at iteration 100 is 0.03246431320895483\t Test cost is 0.12058411963062983\n",
      "TRAIN DEBUG : Cost at iteration 200 is 0.0008415809008837283\t Test cost is 0.13022722128331993\n",
      "TRAIN DEBUG : Cost at iteration 300 is 0.020015040395269804\t Test cost is 0.23823732724556285\n",
      "TRAIN DEBUG : Cost at iteration 400 is 0.17154214951687982\t Test cost is 0.2298424583362956\n",
      "TRAIN DEBUG : Cost at iteration 500 is 0.01668090071885777\t Test cost is 0.1694091626696447\n",
      "TRAIN DEBUG : Cost at iteration 600 is 8.286084854799243e-05\t Test cost is 0.13801487089762712\n",
      "TRAIN DEBUG : Cost at iteration 700 is 4.191702505438052e-05\t Test cost is 0.18575302492305923\n",
      "TRAIN DEBUG : Cost at iteration 800 is 0.16069023067914773\t Test cost is 0.2272268974175718\n",
      "TRAIN DEBUG : Cost at iteration 900 is 0.0014899239258675133\t Test cost is 0.17979358490651054\n",
      "TRAIN DEBUG : Cost at iteration 1000 is 5.3681554454543696e-05\t Test cost is 0.23335611149664914\n",
      "TRAIN DEBUG : Cost at iteration 1100 is 5.443139791389234e-05\t Test cost is 0.17255434671702302\n",
      "TRAIN DEBUG : Cost at iteration 1200 is 0.1085034765710447\t Test cost is 0.16899815780622265\n",
      "TRAIN DEBUG : Cost at iteration 1300 is 0.003885926525391793\t Test cost is 0.16019186221952184\n",
      "TRAIN DEBUG : Cost at iteration 1400 is 0.00011103066189965878\t Test cost is 0.24111435435449455\n",
      "TRAIN DEBUG : Cost at iteration 1500 is 2.987267208460554e-05\t Test cost is 0.14619296051496342\n",
      "TRAIN DEBUG : Cost at iteration 1600 is 0.07841226440657699\t Test cost is 0.12862101419325372\n",
      "TRAIN DEBUG : Cost at iteration 1700 is 0.0005823076460178563\t Test cost is 0.10271002275162652\n",
      "TRAIN DEBUG : Cost at iteration 1800 is 1.087756181913087e-05\t Test cost is 0.19254199769673222\n",
      "TRAIN DEBUG : Cost at iteration 1900 is 1.0672181792593104e-05\t Test cost is 0.15700173876901455\n",
      "TRAIN DEBUG : Cost at iteration 2000 is 0.05276263748451153\t Test cost is 0.12406828757376782\n",
      "TRAIN DEBUG : Cost at iteration 2100 is 0.003257552842208067\t Test cost is 0.13683365221541383\n",
      "TRAIN DEBUG : Cost at iteration 2200 is 6.219301971155653e-05\t Test cost is 0.11726944410144105\n",
      "TRAIN DEBUG : Cost at iteration 2300 is 1.836844329705349e-05\t Test cost is 0.13570303146709004\n",
      "TRAIN DEBUG : Cost at iteration 2400 is 0.13763470689183113\t Test cost is 0.12950316942426054\n",
      "TRAIN DEBUG : Cost at iteration 2500 is 0.0006818442002261692\t Test cost is 0.10121982233322417\n",
      "TRAIN DEBUG : Cost at iteration 2600 is 4.32016656340418e-05\t Test cost is 0.14947487596698522\n",
      "TRAIN DEBUG : Cost at iteration 2700 is 9.84468065280403e-06\t Test cost is 0.1353336540279307\n",
      "TRAIN DEBUG : Cost at iteration 2800 is 0.10144997768046823\t Test cost is 0.09908414396925957\n",
      "TRAIN DEBUG : Cost at iteration 2900 is 4.4608712263288025e-06\t Test cost is 0.1576814554370629\n",
      "TRAIN DEBUG : Cost at iteration 3000 is 2.451663674152446e-05\t Test cost is 0.2491552532939284\n",
      "TRAIN DEBUG : Cost at iteration 3100 is 1.4116253669298468e-05\t Test cost is 0.17149336855043673\n",
      "TRAIN DEBUG : Cost at iteration 3200 is 0.07674102291088498\t Test cost is 0.12314914258876068\n",
      "TRAIN DEBUG : Cost at iteration 3300 is 0.00010862371793388317\t Test cost is 0.17159580214672646\n",
      "TRAIN DEBUG : Cost at iteration 3400 is 7.982842041879598e-05\t Test cost is 0.13907261948937324\n",
      "TRAIN DEBUG : Cost at iteration 3500 is 3.82058598286962e-06\t Test cost is 0.1965093427980817\n",
      "TRAIN DEBUG : Cost at iteration 3600 is 0.061898080625045246\t Test cost is 0.1369150401987791\n",
      "TRAIN DEBUG : Cost at iteration 3700 is 6.528700215365257e-05\t Test cost is 0.18280722167778116\n",
      "TRAIN DEBUG : Cost at iteration 3800 is 1.6312845995976983e-05\t Test cost is 0.15473723192574554\n",
      "TRAIN DEBUG : Cost at iteration 3900 is 1.0093380825544954e-05\t Test cost is 0.15695940839482309\n",
      "TRAIN DEBUG : Cost at iteration 4000 is 0.06388012024355934\t Test cost is 0.1288659989519744\n",
      "TRAIN DEBUG : Cost at iteration 4100 is 7.55109877781411e-05\t Test cost is 0.15595922736529802\n",
      "TRAIN DEBUG : Cost at iteration 4200 is 1.4912852254685497e-05\t Test cost is 0.14899177845348296\n",
      "TRAIN DEBUG : Cost at iteration 4300 is 1.9502650766745353e-05\t Test cost is 0.13604799022566158\n",
      "TRAIN DEBUG : Cost at iteration 4400 is 0.05987012252334532\t Test cost is 0.1292177843625977\n",
      "TRAIN DEBUG : Cost at iteration 4500 is 0.00015253466225510212\t Test cost is 0.11436672399799094\n",
      "TRAIN DEBUG : Cost at iteration 4600 is 9.623334752690642e-06\t Test cost is 0.07737769507957364\n",
      "TRAIN DEBUG : Cost at iteration 4700 is 5.48086607144104e-06\t Test cost is 0.13859609647521487\n",
      "TRAIN DEBUG : Cost at iteration 4800 is 0.09522621884919373\t Test cost is 0.16663629805624355\n",
      "TRAIN DEBUG : Cost at iteration 4900 is 1.4483965928043171e-05\t Test cost is 0.15242471497813567\n",
      "TRAIN DEBUG : Cost at iteration 5000 is 5.808909016422489e-06\t Test cost is 0.13224710703470074\n",
      "TRAIN DEBUG : Cost at iteration 5100 is 4.887055044163544e-06\t Test cost is 0.1813648748900233\n",
      "TRAIN DEBUG : Cost at iteration 5200 is 0.05410174168440331\t Test cost is 0.2004778908061947\n",
      "TRAIN DEBUG : Cost at iteration 5300 is 1.6180588755272258e-05\t Test cost is 0.13088731626621733\n",
      "TRAIN DEBUG : Cost at iteration 5400 is 1.9239564379253555e-06\t Test cost is 0.13209469528681408\n",
      "TRAIN DEBUG : Cost at iteration 5500 is 7.355982825272028e-07\t Test cost is 0.14112908182690656\n",
      "TRAIN DEBUG : Cost at iteration 5600 is 0.04285222493893792\t Test cost is 0.12795940176683224\n",
      "TRAIN DEBUG : Cost at iteration 5700 is 2.0913453323754177e-05\t Test cost is 0.1105191199698621\n",
      "TRAIN DEBUG : Cost at iteration 5800 is 3.948393135447002e-06\t Test cost is 0.12283157317336281\n",
      "TRAIN DEBUG : Cost at iteration 5900 is 1.0129416756382857e-05\t Test cost is 0.14862915694866322\n",
      "TRAIN DEBUG : Cost at iteration 6000 is 0.09102414846534752\t Test cost is 0.09551170015131741\n",
      "TRAIN DEBUG : Cost at iteration 6100 is 1.3352019511853586e-05\t Test cost is 0.14737304173511612\n",
      "TRAIN DEBUG : Cost at iteration 6200 is 1.442429473035731e-05\t Test cost is 0.17649657050458512\n",
      "TRAIN DEBUG : Cost at iteration 6300 is 1.1849705249416224e-05\t Test cost is 0.1517993080248542\n",
      "TRAIN DEBUG : Cost at iteration 6400 is 0.09640912689966062\t Test cost is 0.15021844144019902\n",
      "TRAIN DEBUG : Cost at iteration 6500 is 0.00011751550463841182\t Test cost is 0.17884398030714554\n",
      "TRAIN DEBUG : Cost at iteration 6600 is 0.0024425801103149197\t Test cost is 0.16026335319480248\n",
      "TRAIN DEBUG : Cost at iteration 6700 is 2.1181320310774314e-06\t Test cost is 0.15625504947806454\n",
      "TRAIN DEBUG : Cost at iteration 6800 is 0.07116219443189001\t Test cost is 0.1890084082159755\n",
      "TRAIN DEBUG : Cost at iteration 6900 is 3.209066562715553e-05\t Test cost is 0.17232390077349174\n",
      "TRAIN DEBUG : Cost at iteration 7000 is 5.1739081735688695e-06\t Test cost is 0.1754418893227258\n",
      "TRAIN DEBUG : Cost at iteration 7100 is 8.929765228024364e-07\t Test cost is 0.17202771757060478\n",
      "TRAIN DEBUG : Cost at iteration 7200 is 0.042258346408454835\t Test cost is 0.14153369335992358\n",
      "TRAIN DEBUG : Cost at iteration 7300 is 7.2625660509480704e-06\t Test cost is 0.15277815082386553\n",
      "TRAIN DEBUG : Cost at iteration 7400 is 1.1652741999528187e-05\t Test cost is 0.13703032757187644\n",
      "TRAIN DEBUG : Cost at iteration 7500 is 3.318778380761555e-06\t Test cost is 0.09738341199666775\n",
      "TRAIN DEBUG : Cost at iteration 7600 is 0.0585665234988031\t Test cost is 0.11424685487743054\n",
      "TRAIN DEBUG : Cost at iteration 7700 is 6.497808687402918e-06\t Test cost is 0.12749669238382239\n",
      "TRAIN DEBUG : Cost at iteration 7800 is 2.7419713669521902e-06\t Test cost is 0.11977009017480945\n",
      "TRAIN DEBUG : Cost at iteration 7900 is 1.3263011755815991e-06\t Test cost is 0.12251527195241188\n",
      "TRAIN DEBUG : Cost at iteration 8000 is 0.07511112133119524\t Test cost is 0.13603334165491446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DEBUG : Cost at iteration 8100 is 3.144239941724697e-06\t Test cost is 0.15854103276377657\n",
      "TRAIN DEBUG : Cost at iteration 8200 is 2.748174905871789e-07\t Test cost is 0.15827251843492482\n",
      "TRAIN DEBUG : Cost at iteration 8300 is 9.726201267319404e-07\t Test cost is 0.15689047135717285\n",
      "TRAIN DEBUG : Cost at iteration 8400 is 0.055233211372745804\t Test cost is 0.1666254602134053\n",
      "TRAIN DEBUG : Cost at iteration 8500 is 5.6612108864932756e-06\t Test cost is 0.14158907559456801\n",
      "TRAIN DEBUG : Cost at iteration 8600 is 2.4099526247512707e-06\t Test cost is 0.17754858566275217\n",
      "TRAIN DEBUG : Cost at iteration 8700 is 2.5615080289629353e-06\t Test cost is 0.16499940835823815\n",
      "TRAIN DEBUG : Cost at iteration 8800 is 0.03522365974285656\t Test cost is 0.16268588757200597\n",
      "TRAIN DEBUG : Cost at iteration 8900 is 4.942096297337791e-07\t Test cost is 0.14201240480763286\n",
      "TRAIN DEBUG : Cost at iteration 9000 is 4.924472886547318e-06\t Test cost is 0.1601370915428036\n",
      "TRAIN DEBUG : Cost at iteration 9100 is 3.5969667642224827e-06\t Test cost is 0.1444363278526702\n",
      "TRAIN DEBUG : Cost at iteration 9200 is 0.004746599446040765\t Test cost is 0.15458204873005763\n",
      "TRAIN DEBUG : Cost at iteration 9300 is 7.074752013765534e-06\t Test cost is 0.14609808205752756\n",
      "TRAIN DEBUG : Cost at iteration 9400 is 2.296851602655553e-06\t Test cost is 0.13856343530892018\n",
      "TRAIN DEBUG : Cost at iteration 9500 is 1.2363573461496707e-06\t Test cost is 0.17150399777301567\n",
      "TRAIN DEBUG : Cost at iteration 9600 is 0.005189869398520863\t Test cost is 0.1830580680172691\n",
      "TRAIN DEBUG : Cost at iteration 9700 is 3.856824107386294e-06\t Test cost is 0.16887815965242606\n",
      "TRAIN DEBUG : Cost at iteration 9800 is 9.604953226876615e-07\t Test cost is 0.1701696236928815\n",
      "TRAIN DEBUG : Cost at iteration 9900 is 5.623343128324291e-07\t Test cost is 0.13663524270184318\n",
      "TRAIN DEBUG : Cost at iteration 10000 is 0.05256821300197674\t Test cost is 0.15655296963501986\n",
      "TRAIN DEBUG : Cost at iteration 10100 is 8.111225633453347e-07\t Test cost is 0.14178046561979307\n",
      "TRAIN DEBUG : Cost at iteration 10200 is 1.020362156247429e-06\t Test cost is 0.18442113082875855\n",
      "TRAIN DEBUG : Cost at iteration 10300 is 1.1835850106474909e-06\t Test cost is 0.1654673842719063\n",
      "TRAIN DEBUG : Cost at iteration 10400 is 0.05637839029291049\t Test cost is 0.14890712127944059\n",
      "TRAIN DEBUG : Cost at iteration 10500 is 1.4174788612959628e-05\t Test cost is 0.1581849775982342\n",
      "TRAIN DEBUG : Cost at iteration 10600 is 1.7703538189004723e-06\t Test cost is 0.1616194597752735\n",
      "TRAIN DEBUG : Cost at iteration 10700 is 2.6415539764541826e-06\t Test cost is 0.15971178826324015\n",
      "TRAIN DEBUG : Cost at iteration 10800 is 0.030157768348763676\t Test cost is 0.17765382309599806\n",
      "TRAIN DEBUG : Cost at iteration 10900 is 1.993685220915457e-06\t Test cost is 0.15730545959172806\n",
      "TRAIN DEBUG : Cost at iteration 11000 is 1.3152436811177449e-06\t Test cost is 0.1621536558749553\n",
      "TRAIN DEBUG : Cost at iteration 11100 is 7.510259019748381e-07\t Test cost is 0.15955382332207593\n",
      "TRAIN DEBUG : Cost at iteration 11200 is 0.005665995378377878\t Test cost is 0.16155742309507165\n",
      "TRAIN DEBUG : Cost at iteration 11300 is 5.227783124806852e-07\t Test cost is 0.17289550366459455\n",
      "TRAIN DEBUG : Cost at iteration 11400 is 2.70449682247771e-07\t Test cost is 0.1581869963066221\n",
      "TRAIN DEBUG : Cost at iteration 11500 is 5.929373745835862e-07\t Test cost is 0.13909735511698576\n",
      "TRAIN DEBUG : Cost at iteration 11600 is 0.037503580232707286\t Test cost is 0.1564239715627028\n",
      "TRAIN DEBUG : Cost at iteration 11700 is 1.8028170931293913e-06\t Test cost is 0.16723284698879043\n",
      "TRAIN DEBUG : Cost at iteration 11800 is 5.264572328225371e-07\t Test cost is 0.1553681712412136\n",
      "TRAIN DEBUG : Cost at iteration 11900 is 2.0328396697152534e-07\t Test cost is 0.1670117962888941\n",
      "TRAIN DEBUG : Cost at iteration 12000 is 0.018122832708548284\t Test cost is 0.182947047408286\n",
      "TRAIN DEBUG : Cost at iteration 12100 is 1.5067438684601602e-06\t Test cost is 0.18531774629393097\n",
      "TRAIN DEBUG : Cost at iteration 12200 is 8.110473110277034e-07\t Test cost is 0.1825846631839671\n",
      "TRAIN DEBUG : Cost at iteration 12300 is 3.2375871273128883e-07\t Test cost is 0.17429531056834252\n",
      "TRAIN DEBUG : Cost at iteration 12400 is 0.012924835033478953\t Test cost is 0.1700844735346943\n",
      "TRAIN DEBUG : Cost at iteration 12500 is 2.0666729782556156e-07\t Test cost is 0.15844970147837317\n",
      "TRAIN DEBUG : Cost at iteration 12600 is 9.752370544252854e-07\t Test cost is 0.14219966986100452\n",
      "TRAIN DEBUG : Cost at iteration 12700 is 3.6278904375140685e-07\t Test cost is 0.1424706436430546\n",
      "TRAIN DEBUG : Cost at iteration 12800 is 0.0005840368866668855\t Test cost is 0.1500080139517851\n",
      "TRAIN DEBUG : Cost at iteration 12900 is 8.600443004715677e-07\t Test cost is 0.1582284892011893\n",
      "TRAIN DEBUG : Cost at iteration 13000 is 9.964018627431963e-07\t Test cost is 0.1469968473755706\n",
      "TRAIN DEBUG : Cost at iteration 13100 is 1.2186196365389181e-07\t Test cost is 0.15090049693509977\n",
      "TRAIN DEBUG : Cost at iteration 13200 is 0.0024028646504124286\t Test cost is 0.14335029298830237\n",
      "TRAIN DEBUG : Cost at iteration 13300 is 4.1664123710600807e-07\t Test cost is 0.12185016255925135\n",
      "TRAIN DEBUG : Cost at iteration 13400 is 1.8926854450359061e-07\t Test cost is 0.22943100479241677\n",
      "TRAIN DEBUG : Cost at iteration 13500 is 3.9723207476133304e-07\t Test cost is 0.1470657804619268\n",
      "TRAIN DEBUG : Cost at iteration 13600 is 0.01664917228122341\t Test cost is 0.15583342916181092\n",
      "TRAIN DEBUG : Cost at iteration 13700 is 5.090647060574539e-07\t Test cost is 0.144787382096372\n",
      "TRAIN DEBUG : Cost at iteration 13800 is 1.1050594195134132e-07\t Test cost is 0.15063684991345386\n",
      "TRAIN DEBUG : Cost at iteration 13900 is 9.623524614858421e-08\t Test cost is 0.17069016983841037\n",
      "TRAIN DEBUG : Iterations have stopped at 13901\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105836438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DEBUG : 7 mismatches found at indices [ 6  7  9 12 16 17 18]\n",
      "TEST DEBUG : Testing image at index 6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-51f1d43bf1c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Test segment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     error_test_set(X_test, Y_test, params, activations,\n\u001b[0;32m--> 114\u001b[0;31m                    show_mismatch_images=bool(net_config[\"testing\"][\"show_mismatches\"]))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-7311b25d3f8c>\u001b[0m in \u001b[0;36merror_test_set\u001b[0;34m(test_x, test_y, params, activations, threshold, show_mismatch_images)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TEST DEBUG : Testing image at index {i}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m47\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m38\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             cv.imshow(\"Index {i}, Y = {y}, A = {a}\".format(\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             ), img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "# Network configurations dictionary\n",
    "net_config = {\n",
    "    \"data\": {\n",
    "        \"dir_name\": \"../../Data/5_Fingers\",\n",
    "        \"x_name\": \"X.npy\",\n",
    "        \"y_name\": \"Y.npy\",\n",
    "        \"one_hot_index\": -1,\n",
    "        \"dist_train_dev_test\": (900, 0, 21)\n",
    "    },\n",
    "    \"nn_arc\": {\n",
    "        \"layers\": (100, 50, 50, 50, 5, 1),\n",
    "        \"activations\": [tanh, tanh, relu, relu, relu, sigmoid]\n",
    "    },\n",
    "    \"hyperparameters\": {    # For tuning\n",
    "        \"training_iterations\": 100,\n",
    "        \"iter_limit\": 13901,\n",
    "        \"debug_num_iter\": 100,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"regularization_parameter\": 0.01,\n",
    "        \"num_epochs\": 80,\n",
    "        \"mini_batch_size\": 30,\n",
    "        \"threshold\": 0.9,\n",
    "        \"dropoff_keep\": 1.0\n",
    "    },\n",
    "    \"testing\": {\n",
    "        \"test_image\": \"../../Data/5_Fingers/Test.jpg\",  # Needed\n",
    "        \"false_examples\": \"../../DataCollector/Mask_data_collector/Data_Distributions_Backups/\"\n",
    "                          \"Data_Distribution_5_Fingers/Data_Distribution_Generated/False\",  # No need for this\n",
    "        \"true_examples\": \"../../DataCollector/Mask_data_collector/Data_Distributions_Backups/\"\n",
    "                         \"Data_Distribution_5_Fingers/Data_Distribution_Generated/True\",    # No need for this\n",
    "        \"show_mismatches\": True,  # No need for this\n",
    "        \"live_trial_mode\": True   # No need for this\n",
    "    }\n",
    "}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data into memory\n",
    "    X, Y = load_dataset(dataset_dir_name=net_config[\"data\"][\"dir_name\"], x_name=net_config[\"data\"][\"x_name\"],\n",
    "                        y_name=net_config[\"data\"][\"y_name\"], one_hot_index=net_config[\"data\"][\"one_hot_index\"])\n",
    "    datasets = split_train_dev_test(X, Y, net_config[\"data\"][\"dist_train_dev_test\"])\n",
    "    X_train, Y_train = datasets[\"train\"]\n",
    "    X_dev, Y_dev = datasets[\"dev\"]\n",
    "    X_test, Y_test = datasets[\"test\"]\n",
    "    # Load weights and activation functions\n",
    "    nn_architecture = {\n",
    "        \"layers\": net_config[\"nn_arc\"][\"layers\"],\n",
    "        \"activations\": net_config[\"nn_arc\"][\"activations\"]\n",
    "    }\n",
    "    architecture_nn, params = init_params_deep(X.shape[0], nn_architecture[\"layers\"])\n",
    "    activations = nn_architecture[\"activations\"]\n",
    "    # Training the network\n",
    "    # Hyperparameters\n",
    "    num_iter = net_config[\"hyperparameters\"][\"training_iterations\"]\n",
    "    debug_iter_num = net_config[\"hyperparameters\"][\"debug_num_iter\"]\n",
    "    learning_rate = net_config[\"hyperparameters\"][\"learning_rate\"]\n",
    "    reg_param_lambda = net_config[\"hyperparameters\"][\"regularization_parameter\"]\n",
    "    num_epochs = net_config[\"hyperparameters\"][\"num_epochs\"]\n",
    "    mini_batch_size = net_config[\"hyperparameters\"][\"mini_batch_size\"]\n",
    "    dropoff_prob = net_config[\"hyperparameters\"][\"dropoff_keep\"]\n",
    "    cost_tracker = {\n",
    "        \"train_x\": [],\n",
    "        \"train_cost\": [],\n",
    "        \"eval_x\": [],\n",
    "        \"eval_cost\": []\n",
    "    }\n",
    "    print(\"TRAIN DEBUG : {it} training iterations required, iteration limit is {iter_limit}\".format(\n",
    "        it=int(num_iter * num_epochs * X_train.shape[1] / mini_batch_size),\n",
    "        iter_limit=net_config[\"hyperparameters\"][\"iter_limit\"]\n",
    "    ))\n",
    "\n",
    "    # Main training process\n",
    "    try:\n",
    "        for train_iter_num in range(num_iter):\n",
    "            X_training, Y_training = shuffle_dataset(X_train, Y_train)\n",
    "            (X_training_batches, Y_training_batches) = divide_into_mini_batches(X_training, Y_training, mini_batch_size)\n",
    "            num_batches = len(Y_training_batches)\n",
    "            for batch_index in range(len(Y_training_batches)):\n",
    "                X_training_batch = X_training_batches[batch_index]\n",
    "                Y_training_batch = Y_training_batches[batch_index]\n",
    "                for epoch_count in range(num_epochs):\n",
    "                    # Forward Propagate\n",
    "                    Y_pred_mini_batch, cache_mini_batch = forward_propagate_deep(params, activations,\n",
    "                                                                                 X_training_batch)\n",
    "                    # Note the cost\n",
    "                    cost_iter = cost_function(Y_pred_mini_batch, Y_training_batch)\n",
    "                    i = train_iter_num * num_batches * num_epochs + batch_index * num_epochs + epoch_count\n",
    "                    cost_tracker[\"train_x\"].append(i)\n",
    "                    cost_tracker[\"train_cost\"].append(cost_iter)\n",
    "                    if i % debug_iter_num == 0 or i == 0:\n",
    "                        pred_test, _ = forward_propagate_deep(params, activations, X_test)\n",
    "                        eval_cost = cost_function(pred_test, Y_test)\n",
    "                        cost_tracker[\"eval_x\"].append(i)\n",
    "                        cost_tracker[\"eval_cost\"].append(eval_cost)\n",
    "                        print(\"TRAIN DEBUG : Cost at iteration {it_num} is {cost}\\t Test cost is {test_cost}\".format(\n",
    "                            cost=cost_iter, it_num=i, test_cost=eval_cost))\n",
    "                    # Back propagation\n",
    "                    params = back_propagation_deep(cache_mini_batch, params, activations, Y_training_batch,\n",
    "                                                   learning_rate, reg_param_lambda)\n",
    "                    if net_config[\"hyperparameters\"][\"iter_limit\"] is not None:\n",
    "                        if i == net_config[\"hyperparameters\"][\"iter_limit\"]:\n",
    "                            raise StopIteration\n",
    "    except StopIteration:\n",
    "        print(\"TRAIN DEBUG : Iterations have stopped at {i_num}\".format(i_num=i))\n",
    "\n",
    "    # print(params)\n",
    "    # print(forward_propagate_deep(params, activations, X))\n",
    "\n",
    "    plt.plot(cost_tracker[\"train_x\"], cost_tracker[\"train_cost\"], 'b-',\n",
    "             cost_tracker[\"eval_x\"], cost_tracker[\"eval_cost\"], 'g-')\n",
    "    plt.legend([\"Training\", \"Evaluation\"])\n",
    "    plt.show()\n",
    "    # Test segment\n",
    "    error_test_set(X_test, Y_test, params, activations,\n",
    "                   show_mismatch_images=bool(net_config[\"testing\"][\"show_mismatches\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
