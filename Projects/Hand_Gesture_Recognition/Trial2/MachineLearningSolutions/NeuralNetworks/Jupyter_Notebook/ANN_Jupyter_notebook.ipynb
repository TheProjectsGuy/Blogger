{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A neural network, sometimes also referred as multi layer perceptron, is basically an algorithm that is used to learn more sophisticated features than what a perceptron can learn. It does this by using multiple layers of perceptrons. The image below explains a little about the architecture it follows\n",
    "<center> **Neural Network** </center>\n",
    "![Neural Network architecture](NN_demo_image.jpeg \"A Neural Network\")\n",
    "\n",
    "The inputs are given to the *input layer*, the hidden layers simply perform a weighed sum of the outputs of the previous layer, add it with a bias and then pass it through a function. Usually, the same activation function is used for every neuron in one layer. If you've gone through my perceptron notebook (link [here](https://github.com/TheProjectsGuy/Blogger/blob/master/Projects/Hand_Gesture_Recognition/Trial2/MachineLearningSolutions/Perceptron/Jupyter_Notebook/Perceptron_model_5_fingers_detection.ipynb)), you can perceive that every neuron is basically acting like a perceptron (hence the name _multi layer perceptron_). All the neurons are connected by a _weight_, this is used to perform a weighed sum (same as in the perceptron)\n",
    "\n",
    "Basically to summarize what happens in a neural network, \n",
    "- *Neurons of input layer*\n",
    "   - Simply take the inputs\n",
    "- *Neurons of hidden layers*\n",
    "   - Take values of the previous payer\n",
    "   - Perform a weighed sum\n",
    "   - Add a bias\n",
    "   - Pass the result through a function (known as the activation function of the layer)\n",
    "   - Store the output\n",
    "- *Neurons of output layer*\n",
    "   - Take the values of the previous layer\n",
    "   - Perform a weighed sum\n",
    "   - Add a bias\n",
    "   - Pass the result through a function\n",
    "   - Present the output. This is the _output of the neural network_.\n",
    "\n",
    "The above points are exactly what we do in forward propagation through the neural network. In order to learn the proper weights connecting the neurons from layer _l_ to _l + 1_, we perform something called as _Back Propagation_. In this, we apply the concept of gradient descent, chain rule of differentiation and a little matrice calculus to obtain the changes we need to make.\n",
    "\n",
    "In the [perceptron](https://github.com/TheProjectsGuy/Blogger/blob/master/Projects/Hand_Gesture_Recognition/Trial2/MachineLearningSolutions/Perceptron/Jupyter_Notebook/Perceptron_model_5_fingers_detection.ipynb) algorithm, we used the entire training set in order to take only one step, and then took several iterations of it. Here, we'll try something different. We'll divide the training set into many smaller training sets (known as batches), and then perform training on batches. This is known as _mini batch gradient descent_. We'll program that too. We'll not use any of the modern programming frameworks like [_tensorflow_](https://www.tensorflow.org/), [_scikit learn_](http://scikit-learn.org/stable/index.html), or anything. We'll make one from the basics to understand how everything works in a simple neural network. So let's get started\n",
    "\n",
    "## _Hand Gesture Recognition_\n",
    "Here are the different hand gestures we'll be working with. For this, we'll follow the same thing we did in the perceptron algorithm. We'll train a neural network to detect the five finger gesture.\n",
    "<center> **Gestures** </center>\n",
    "![Gestures](Hand_Gesture_Classes_5_Finger.jpg)\n",
    "Usually, there's an intuition in deciding the number of layers. We can attach them wth minor features but we're free to explore possibilities.\n",
    "\n",
    "Let's start by importing the needed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np                     # For optimised mathematical operations\n",
    "from matplotlib import pyplot as plt   # For plotting the results \n",
    "\n",
    "np.random.seed(2)       # Just so that we get the same random number every time we run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and shuffling data\n",
    "Now we're dealing with a supervised learning problem (labelled dataset needed). For this neural network, we have a dataset consisting of binary images (every pixel is either white or black). The images are actually 235 by 190, but they're shrinked to 47 by 38 size, so that we have smaller number of input featres to deal with (1786 input features). Fortunately, we have a **.npy** file for inputs and labelled outputs, so all we need to do is to load the binary data file.\n",
    "Usually, we are unaware of the source of data, so it's always a good idea to shuffle it. This is done so that we obtain a uniform distribution of data, this improves overall performance. The dataset we are using has about a 50-50 distribution (50 % images are images of 5 fingers), shuffling it will ensure uniform distribution.\n",
    "\n",
    "### Splitting data into buckets\n",
    "To train and evaluate the model, we usually split the data into three groupt (buckets), **train**, **dev** and **test** sets. Their purpose is stated below\n",
    "\n",
    "Bucket name | Purpose\n",
    "-----|------\n",
    "**Train** | Used to train a model\n",
    "**Dev** | Used to compare different models (also known as the cross validation set)\n",
    "**Test** | Used to perform the final test of the model\n",
    "\n",
    "While training, we split the training set into various parts (batches), let us implement a function for that as well.\n",
    "\n",
    "So first, *loading the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(dataset_dir_name=\"Data\", x_name=\"X.npy\",\n",
    "                 y_name=\"Y_one_hot_encoded.npy\", one_hot_index=0, shuffle_data=True, normalize_data=True):\n",
    "    \"\"\"\n",
    "    Loads a dataset stored as a .npy file\n",
    "    :param dataset_dir_name: Name of the folder in which data is\n",
    "    :param x_name: Name of file which contains inputs (with .npy extension)\n",
    "    :param y_name: Name of file which contains outputs (with .npy extension)\n",
    "    :param one_hot_index: The index you're training for (pass -1 for passing raw output file), should be >= -1\n",
    "    :param shuffle_data: Shuffle the data\n",
    "    :param normalize_data: Normalize the input data\n",
    "    :return:\n",
    "        X, Y\n",
    "        X -> Inputs\n",
    "        Y -> Outputs\n",
    "        The output will be shuffled if shuffle_data is True, else it won't be shuffled\n",
    "    \"\"\"\n",
    "    # Load the dataset from memory\n",
    "    X = np.load(\"../{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                 f_name=x_name))\n",
    "    print(\"DATA DEBUG : Inputs shape is {in_shape}\".format(in_shape=X.shape))\n",
    "    if one_hot_index == -1:\n",
    "        # Parse the entire dataset as it is\n",
    "        Y = np.load(\"../{main_root}/{f_name}\".format(main_root=dataset_dir_name,\n",
    "                                                     f_name=y_name))\n",
    "    elif one_hot_index >= 0:\n",
    "        # Get the particular one_hot_encoded row\n",
    "        Y_one_hot = np.load(\"../{main_root}/{f_name}\".format(\n",
    "            main_root=dataset_dir_name, f_name=y_name\n",
    "        ))\n",
    "        Y = np.array(Y_one_hot[one_hot_index, :].reshape((1, -1)))\n",
    "    else:\n",
    "        raise IndexError(\"The index {ind_number} is an illegal index\".format(ind_number=one_hot_index))\n",
    "    print(\"DATA DEBUG : Output shape is {out_shape}\".format(out_shape=Y.shape))\n",
    "    if normalize_data:\n",
    "        X = X / 255\n",
    "    if shuffle_data:\n",
    "        X, Y = shuffle_dataset(X, Y)\n",
    "    Y_true_p = Y.nonzero()[1].reshape(1, -1).shape[1] / Y.shape[1]\n",
    "    print(\"DATA DEBUG : {tp}% data is true\".format(tp=Y_true_p * 100))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "def shuffle_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    Shuffles the dataset (shuffle columns) and returns it\n",
    "    :param X : Inputs\n",
    "    :param Y : Outputs\n",
    "    :return:\n",
    "        tuple(X, Y)\n",
    "        X, Y with their columns shuffled\n",
    "    \"\"\"\n",
    "    buffer_data = np.row_stack((X, Y))                        # Unify the inputs and outputs of data\n",
    "    buffer_data = buffer_data.T                               # columns -> rows\n",
    "    np.random.shuffle(buffer_data)                            # Performs row shuffling\n",
    "    buffer_data = buffer_data.T                               # rows -> columns\n",
    "    X = buffer_data[0:X.shape[0], :]                          # Retrieve X\n",
    "    Y = buffer_data[X.shape[0]:X.shape[0] + Y.shape[0], :]    # Retrieve Y\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to split the dataset into **train**, **dev** and **test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "def split_train_dev_test(X, Y, sizes=(2500, 136, 136)):\n",
    "    \"\"\"\n",
    "    Split the dataset into train, dev and test sets\n",
    "    :param X: Inputs\n",
    "    :param Y: Outputs\n",
    "    :param sizes: Distribution tuple\n",
    "    :return: Dictionary\n",
    "        \"train\" : (X_train, Y_train)\n",
    "        \"dev\" : (X_dev, Y_dev)\n",
    "        \"test\" : (X_test, Y_test)\n",
    "    \"\"\"\n",
    "    # Split the dataset\n",
    "    # Train sets\n",
    "    X_train = X[:, 0:sizes[0]]\n",
    "    Y_train = Y[:, 0:sizes[0]]\n",
    "    # Dev set\n",
    "    X_dev = X[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    Y_dev = Y[:, sizes[0]:sizes[0] + sizes[1]]\n",
    "    # Test set\n",
    "    X_test = X[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    Y_test = Y[:, sizes[0] + sizes[1]:sizes[0] + sizes[1] + sizes[2]]\n",
    "    print(\"DATA DEBUG : Train input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_train.shape, out_shape=Y_train.shape\n",
    "    ))\n",
    "    print(\"DATA DEBUG : Test input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_test.shape, out_shape=Y_test.shape\n",
    "    ))\n",
    "    print(\"DATA DEV : Dev input shape {in_shape}, output shape {out_shape}\".format(\n",
    "        in_shape=X_dev.shape, out_shape=Y_dev.shape\n",
    "    ))\n",
    "    rdict = {\n",
    "        \"train\": (X_train, Y_train),\n",
    "        \"dev\": (X_dev, Y_dev),\n",
    "        \"test\": (X_test, Y_test)\n",
    "    }\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to divide a dataset into batches of particular size (mini batches of *mini_batch_size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the set into mini_batches\n",
    "def divide_into_mini_batches(X, Y, mini_batch_size, debugger_output=False):\n",
    "    \"\"\"\n",
    "    Divides the passes set into mini batches\n",
    "    :param X : Inputs\n",
    "    :param Y : Outputs\n",
    "    :param mini_batch_size : The size of the mini batches required (if it's not an exact fit, then \n",
    "                             the last mini batch won't be of this size)\n",
    "    :param debugger_output : if True then the function will print the status of batch_splitting \n",
    "    :return: Tuple\n",
    "            (X_mini_batches, Y_mini_batches)\n",
    "            X_mini_batches : Containing arrays of input batches\n",
    "            Y_mini_batches : Containing arrays of output batches\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    n_batches = m // mini_batch_size\n",
    "    if debugger_output:\n",
    "        print(\"DATA DEBUG : Dividing {num} examples into batches of size {batch_s}. {n_b} full batches\".format(\n",
    "        num=m, batch_s=mini_batch_size, n_b = n_batches\n",
    "    ))\n",
    "    X_mini_batches = []\n",
    "    Y_mini_batches = []\n",
    "    for i in range(n_batches):\n",
    "        X_mini_batches.append(X[:, i * mini_batch_size : (i + 1) * mini_batch_size])\n",
    "        Y_mini_batches.append(Y[:, i * mini_batch_size : (i + 1) * mini_batch_size])\n",
    "    if m % mini_batch_size != 0:\n",
    "        X_mini_batches.append(X[:, n_batches * mini_batch_size : ])\n",
    "        Y_mini_batches.append(Y[:, n_batches * mini_batch_size : ])\n",
    "    return (X_mini_batches, Y_mini_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the neural network\n",
    "### Declaration\n",
    "Here comes the part where we plan out the architecture of the neural network (also known as the _model architecture_). Let us decide the following before moving further :\n",
    "- Number of hidden layers\n",
    "- Number of neurons in every layer\n",
    "- Activations of every layer\n",
    "\n",
    "The layer, number of neurons, activations and description are shown in the table below\n",
    "\n",
    "| Layer number | Number of neurons | Activation | Description |\n",
    "| ----|----|----|----| \n",
    "| 0 | 1786 | ```None``` | Input layer\n",
    "| 1 | 100 | ```tanh``` | First hidden layer\n",
    "| 2 | 50 | ```tanh``` | Second hidden layer\n",
    "| 3 | 50 | ```relu``` | Third hidden layer\n",
    "| 4 | 50 | ```relu``` | Fourth hidden layer\n",
    "| 5 | 5 | ```relu``` | Fifth hidden layer\n",
    "| 6 | 1 | ```sigmoid``` | Output layer\n",
    "\n",
    "Things to note from above are that the input layer is not counted in the number of layers and it doesn't have any activation function (it simply receives and gives the input to the neural network), the output layer (in case of classification problems) has a sigmoid activation function.\n",
    "\n",
    "### Initialization\n",
    "As discussed earlier, every two consecutive layers have a set of connections, which we'll call **weights**, additionally, every neuron of layers other than input have a **bias** (which is added to the weighed sum). These are called _parameters_ in our code. \n",
    "\n",
    "For the purpose of this notebook, we'll use the following notation\n",
    "\n",
    "- $N^{l}$ means the number of neurons in layer *l*. **L** is the number of layers so $l\\in \\left [ 0,L \\right ]$\n",
    "- $W^{l}$ means the weights connecting layer *l-1* and *l*. Thus it's a matrix of dimension $N^{l}$ rows and $N^{l-1}$ columns\n",
    "    - Further, $W_{i}^{l}$ is a row vector of the connection weights for the *i*<sup>th</sup> neuron of *l*<sup>th</sup> layer. Hence, it's shape (dimension) is 1 row by $N^{l-1}$ columns\n",
    "- $b^{l}$ means the biases for the neurons of the layer *l*. It's shape is *l* rows and 1 column\n",
    "\n",
    "When we initialize the weights, we **don't** set them to 0. This is because we don't want all of them to get updated in the same manner. This is called _breaking the symmetry_ of the neural network. We therefore initialize them to random numbers. It makes sence for the layers having big number of inputs to have smaller weights at initialization, this keeps the results away from extreme. We don't want high values for the weighed sums because if you observe the graphs of _tanh_ and _sigmoid_ functions (below) you can see that their values barely change at extremes (their derivative is 0), this will badly affect the gradient descent algorithm (as we'll see later).\n",
    "<img src=\"sigmoid_tanh_function_graph.png\" alt=\"Activation Functions\" height=200 width=300>\n",
    "\n",
    "There are multiple initialization techniques used, we'll use the one shown below\n",
    "```python\n",
    "params[\"W\" + str(i)] = np.random.randn(layer_size[i], layer_size[i - 1]) * 2 / np.sqrt(layer_size[i - 1])\n",
    "```\n",
    "This is also called the _He Initialization technique_. Let's initialize the parameters according to the above rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters of the neural network\n",
    "def init_params_deep(input_size, layer_tup):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the DNN\n",
    "    :param input_size: Size of the input layer\n",
    "    :param layer_tup:  Tuple of layer sizes (hidden layers + output layer)\n",
    "    :return:\n",
    "        layers_info, parameters\n",
    "        :return layer_size\n",
    "            The size architecture of the neural network. Sizes of every layer including input layer\n",
    "        :return params\n",
    "        Dictionary\n",
    "            \"W + str(i)\" : Weight of layer i\n",
    "            \"b + str(i)\" : Biases of layer i\n",
    "    \"\"\"\n",
    "    # Initialize the neural network with parameters\n",
    "    layer_size = (input_size, *layer_tup)\n",
    "    print(\"DATA DEBUG : Initializing neural network with architecture {arc}\".format(arc=layer_size))\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_size)):\n",
    "        params[\"W\" + str(i)] = np.random.randn(layer_size[i], layer_size[i - 1]) * 2 / np.sqrt(layer_size[i - 1])\n",
    "        params[\"b\" + str(i)] = np.zeros((layer_size[i], 1))\n",
    "    return layer_size, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "An activation function is one of the crucial reasons why a neural network even works. Say we used no activation functions (just performed the weighed sum, added bias and then passed it just as it is), there would be a problem. We'll just be performing a weighed sum over weighed sum over weighed sum, which ultimately boils down to a *single weighed sum* which defies the reason why we even use a neural network, we need it to explore way more features, sophisticated features. \n",
    "\n",
    "Now that we know that using an activation function for every layer is important, the question is what activation function should we use for different layers ?<br>\n",
    "- Well, it depends. If you're doing a classification problem (like we're doing here), it's suggested that the output layer has the ```sigmoid``` function. For regression based problem (guessing a range, eg: housing price prediction), we usually use the ```relu``` function.\n",
    "- For hidden layers, you're free to explore different possibilities\n",
    "\n",
    "We need to implement different functions and their derivatives as well (we'll need the derivatives for gradient descent algorithm). It's a good idea to follow a particular pattern in declaration (so that experimenting is easier). <br>\n",
    "We'll start by making an _activation parser_, which splits the activation function and it's derivative. We do this so that it's easier to handle during forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split activation function into function and it's derivative\n",
    "def parse_activations(activations):\n",
    "    \"\"\"\n",
    "    Parse activations into function and derivatives\n",
    "    :param activations: Function which returns a dictionary\n",
    "        \"function\" : Activation function\n",
    "        \"derivative\" : Derivative of function\n",
    "    :return:\n",
    "        activation_function, activation_function_derivatives\n",
    "    \"\"\"\n",
    "    activation_fncs = []\n",
    "    activation_fnc_der = []\n",
    "    for fnc in activations:\n",
    "        activation_fncs.append(fnc()[\"function\"])\n",
    "        activation_fnc_der.append(fnc()[\"derivative\"])\n",
    "    return activation_fncs, activation_fnc_der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function\n",
    "This is probably the most common activation function used in neural networks. It's graph is shown below\n",
    "<img src=\"sigmoid_function.png\" width=500>\n",
    "It's mathematical formula is \n",
    "$$ sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "It's derivative is \n",
    "$$ \\frac{\\partial sigmoid(x)}{\\partial x} = \\frac{e^{-x}}{(1+e^{-x})^{2}} = sigmoid(x) \\times (1-sigmoid(x)) $$\n",
    "Let's impliment this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Activation function\n",
    "def sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : sigmoid_function\n",
    "            \"derivative\" : sigmoid_derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def sigmoid_function(x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid_function(x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of sigmoid function\n",
    "        :param x:\n",
    "        :return:\n",
    "            sigmoid'(x)\n",
    "        \"\"\"\n",
    "        return sigmoid_function(x) * (1 - sigmoid_function(x))\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": sigmoid_function,\n",
    "        \"derivative\": sigmoid_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation function\n",
    "This is another commonly used activation function for neural networks. It's graph is shown below\n",
    "<img src=\"relu_function.png\" width=400>\n",
    "It's mathematically defined as \n",
    "$$ReLU(x) = \\left\\{\\begin{matrix}\n",
    "0 & x \\leq 0\\\\\n",
    "x & x > 0 \n",
    "\\end{matrix}\\right.$$\n",
    "It's derivative is \n",
    "$$\\frac{\\partial ReLU(x)}{\\partial x} = \\left\\{\\begin{matrix}\n",
    "0 & x \\leq 0\\\\\n",
    "1 & x > 0 \n",
    "\\end{matrix}\\right.$$\n",
    "Let's implement this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu():\n",
    "    \"\"\"\n",
    "    ReLU (Rectified linear unit) activation function\n",
    "    :param x:\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : relu_function\n",
    "            \"derivative\" : relu_derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def relu_function(x):\n",
    "        \"\"\"\n",
    "        ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu(x)\n",
    "        \"\"\"\n",
    "        ret_x = x.copy()\n",
    "        ret_x[ret_x < 0] = 0\n",
    "        return ret_x\n",
    "\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU function\n",
    "        :param x:\n",
    "        :return:\n",
    "            relu'(x)\n",
    "        \"\"\"\n",
    "        ret_d = x.copy()\n",
    "        ret_d[x <= 0] = 0\n",
    "        ret_d[x > 0] = 1\n",
    "        return ret_d\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": relu_function,\n",
    "        \"derivative\": relu_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic tan activation function\n",
    "This is a common function when we want the effects of sigmoid but a little broader than sigmoid (this has values between -1 and 1). It's graph is shown below\n",
    "<img src=\"tanh_function.png\" width=300>\n",
    "It's mathematically defined as \n",
    "$$tanh(x)=\\frac{sinh(x)}{cosh(x)}=\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "Remember that $sinh(x)=\\frac{e^{x}-e^{-x}}{2}$ and $cosh(x)=\\frac{e^{x}+e^{-x}}{2}$\n",
    "\n",
    "The derivative is given by \n",
    "$$\\frac{\\partial tanh(x)}{\\partial x} = \\frac{4 e^{2x}}{(e^{2x}+1)^{2}} = \\left ( \\frac{2}{e^{x} + e^{-x}} \\right )^{2}= \\frac{1}{cosh(x)}$$\n",
    "Let's implement this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh Activation function\n",
    "def tanh():\n",
    "    \"\"\"\n",
    "    Tan Hyperbolic activation function\n",
    "    :return:\n",
    "        Dictionary\n",
    "            \"function\" : Tan hyperbolic function\n",
    "            \"derivative\" : Derivative of tan hyperbolic function\n",
    "    \"\"\"\n",
    "\n",
    "    def tanh_function(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(x):\n",
    "        return np.square(1 / (np.cosh(x)))\n",
    "\n",
    "    func_dict = {\n",
    "        \"function\": tanh_function,\n",
    "        \"derivative\": tanh_derivative\n",
    "    }\n",
    "    return func_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "As discussed in the beginning, forward propagation is the process of getting the predictions from the neual network. We will perform the following from layer 1 to L.\n",
    "1. Take outputs of previous layer\n",
    "2. Perform the weighed sum with the weights matrice\n",
    "3. Add bias\n",
    "4. Pass the result through the activation function of the layer\n",
    "\n",
    "The output is set to the output of the last step.<br>\n",
    "A simple way of doing steps 1 through 3 is to do a matrice multiplication of weights and the outputs of previous layer. Mathematical steps are as follows\n",
    "\n",
    "$$Z^{l} = W^{l} \\times A^{l-1} + b^{l}$$\n",
    "$$A^{l} = G^{l}(Z^{l})$$\n",
    "\n",
    "where $A^{l}$ is the output of layer $l$, $Z^{l}$ is the weighed sum performed for the layer $l$ and $G^{l}(...)$ is the activation function for the layer $l$. Remember that $A^{0}$ is nothing but the input given to the neural network. Also, to perform matrice multiplication, we can use the ```@``` operator in python or use the function ```np.matmul(a,b)```. I've used the latter for clarification. Let's implement this through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation step\n",
    "def forward_propagate_deep(params, activations, input):\n",
    "    \"\"\"\n",
    "    Perform forward propagation of the neural network\n",
    "    :param params: Dictionary of weights and biases of the network\n",
    "            params[\"W\" + str(l)] : Weights of layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: Array of activation functions of every layer\n",
    "    :param input: Inputs given to the neural network\n",
    "    :return:\n",
    "        A_final, cache\n",
    "        A_final : Final output values of the DNN after forward propagation\n",
    "        cache : Dictionary\n",
    "                cache[\"Z\" + str(l)] : The weighed sum\n",
    "                cache[\"A\" + str(l)] : The value after passing the weighed sums through the activation function\n",
    "    \"\"\"\n",
    "    # Forward propagation process\n",
    "    cache = {\"A0\": input}                # This stores all the acivations and weighed sums of the DNN\n",
    "    L = int(len(params.keys()) / 2)      # Number of layers\n",
    "    for i in range(1, L + 1):\n",
    "        cache[\"Z\" + str(i)] = np.matmul(params[\"W\" + str(i)], cache[\"A\" + str(i - 1)]) + params[\"b\" + str(i)]    # Weighed sum performed\n",
    "        act = activations[i - 1]()[\"function\"]              # Activation function of layer i\n",
    "        cache[\"A\" + str(i)] = act(cache[\"Z\" + str(i)])      # Output of layer i\n",
    "    A_final = cache[\"A\" + str(L)]     # Output of the neural network\n",
    "    return A_final, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "Let's know about the back propagation algorithm. This is the actual _magic_ in learning of neural networks. In this, we basically define a _loss metric_ (a cost function), and minimize all the parameters of the neural network with reference to that function. The algorithm that does this is called **Gradient Descent** . <br>\n",
    "Let's see how this algorithm works by assuming only one parameter to adjust (*w*)<br>\n",
    "- We first define a cost function (which is a measure of penalty on *w* for missing it's optimal target), *J(w)*. It is obvious that we want minimum cost for the model, so we need to adjust *w* in such a way that we get minimum cost.\n",
    "- If we differentiate *J* with respect to *w*, we get the slope of a line (tangent). If we move ahead on the tangent, we'll reach the peak, so we need to move backwards. That is, if the slope is +ve, then we need to move towards left, if the slope is -ve, then we need to move towards right.\n",
    "- We'll take a tiny step in the needed direction and perform the same process iteratively until convergence of parameters (till parameters don't change significantly) or for some number of iterations.\n",
    "![Gradient Descent](gradient-descent.png \"Gradient Descent\")\n",
    "This was a brief information about gradient descent, for more you can check [this](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) out.<br>\n",
    "\n",
    "## Mathematical principle of back propagation\n",
    "\n",
    "For one or two parameters, it's easy to visualize what's happening. But, we have a lot more (all the weights and biases are 'parameters' to learn), so let's check the maths behind it<br>\n",
    "We know <br>\n",
    "<center>\n",
    "$W^{l} := W^{l} - \\alpha \\times \\frac{\\partial J}{\\partial W^{l}}$ and $b^{l} := b^{l} - \\alpha \\times \\frac{\\partial J}{\\partial b^{l}}$\n",
    "</center>\n",
    "Here, $\\alpha$ is called the learning rate, it specifies how aggresively we take steps towards the minima.\n",
    "The := implies to calculate the value to the right and assign it to the left variable<br>\n",
    "We define the cost funcion as given below\n",
    "$$J(\\hat{Y}, Y) = - \\frac{1}{m} \\left [\\sum_{j = 1}^{m} Y_{j} \\times ln(\\hat{Y}_{j}) + (1-Y_{j}) \\times ln(1-\\hat{Y}_{j}) \\right ]$$\n",
    "$Y_{j}$ is the label of *j*<sup>th</sup> training example (which is either 0 or 1), and $\\hat{Y}_{j}$ (or $A_{j}$ or $A^{L}_{j}$ is the prediction of our neural network for the *j*<sup>th</sup> training example. It can easily be shown that the above function has very high value when the prediction is the binary inverse of actual value. We take the average over all the training examples for the final cost. We can also write the cost as $J(A^{L})$.<br>\n",
    "**Let's see what happens to the weights of the last layer : $W^{L}$ and $b^{L}$**\n",
    "<center>\n",
    "$W^{L} := W^{L} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial W^{L}}$ and $b^{L} := b^{L} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial b^{L}}$\n",
    "</center>\n",
    "We only need to calculate the value of $\\frac{\\partial J(A^{L})}{\\partial W^{L}}$ to get the change.<br>\n",
    "We can write $\\frac{\\partial J(A^{L})}{\\partial W^{L}}$ as $\\frac{\\partial J(A^{L})}{\\partial A^{L}} \\times \\frac{\\partial A^{L}}{\\partial W^{L}}$ (This is known as chain rule). We can do the following calculations : <br>\n",
    "$\\frac{\\partial J(A^{L})}{\\partial A^{L}} = J'(A^{L})=\\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}$ and $\\frac{\\partial A^{L}}{\\partial W^{L}} = \\frac{\\partial [G^{L}(W^{L}\\times A^{L-1} + b^{L})]}{\\partial W^{L}} = G'^{L}(W^{L}\\times A^{L-1} + b^{L}) \\ast \\left ( A^{L-1} \\right )^{T} = G'^{L}(Z^{L})\\ast \\left ( A^{L-1} \\right )^{T}$. By the multiplying both of these, we get the following \n",
    "$$\\frac{\\partial J(A^{L})}{\\partial W^{L}} = \\left [ \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}\\cdot G'^{L}(Z^{L}) \\right ]\\times \\left(A^{L-1} \\right )^{T}$$\n",
    "Here, $(A^{L-1})^{T}$ is the transpose of $A^{L-1}$<br>\n",
    "Similarly, if we perform the same operations for the bias, we get\n",
    "$$\\frac{\\partial J(A^{L})}{\\partial b^{L}} = \\left [ \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}\\cdot G'^{L}(Z^{L}) \\right ]\\times \\Upsilon_{m,1}$$\n",
    "The $\\Upsilon_{m,1}$ indicates a column vector of shape *m* rows (number of training examples) and 1 column consisting of ones only.<br><br>\n",
    "**Let's see what happens to the weights of the second last layer : $W^{L-1}$ and $b^{L-1}$**\n",
    "<center>\n",
    "$W^{L-1} := W^{L-1} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial W^{L-1}}$ and $b^{L-1} := b^{L-1} - \\alpha \\times \\frac{\\partial J(A^{L})}{\\partial b^{L-1}}$\n",
    "</center>\n",
    "We can write $\\frac{\\partial J(A^{L})}{\\partial W^{L-1}}$ as $\\frac{\\partial J(A^{L})}{\\partial A^{L}} \\times \\frac{\\partial A^{L}}{\\partial W^{L-1}}$, applying the chain rule again gives us $\\frac{\\partial J(A^{L})}{\\partial A^{L}} \\times \\frac{\\partial A^{L}}{\\partial A^{L-1}} \\times \\frac{\\partial A^{L-1}}{\\partial W^{L-1}}$. From this, we know that $\\frac{\\partial J(A^{L})}{\\partial A^{L}} = \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})}$. We only need to find $\\frac{\\partial A^{L}}{\\partial A^{L-1}}$ and $\\frac{\\partial A^{L-1}}{\\partial W^{L-1}}$.<br>\n",
    "Here, we can approach a few standard rules derived below (for any *k*),\n",
    "$$\\frac{\\partial A^{k}}{\\partial A^{k-1}} = \\frac{\\partial G^{k}(Z^{k})}{\\partial A^{k-1}} = G'^{k}(Z^{k})\\cdot \\frac{\\partial (W^{k} \\times A^{k-1} + b^{k})}{\\partial A^{k-1}} = G'^{k}(Z^{k})\\cdot W^{k}$$\n",
    "and\n",
    "$$\\frac{\\partial A^{k}}{\\partial W^{k}} = \\frac{\\partial G^{k}(W^{k} \\times A^{k-1} + b)}{\\partial W^{k}} = \\frac{\\partial G^{k}(Z^{k})}{\\partial W^{k}} = G'^{k}(Z^{k})\\cdot \\frac{\\partial (W^{k} \\times A^{k-1} + b)}{\\partial W^{k}} = G'^{k}(Z^{k})\\cdot A^{k-1}$$\n",
    "This gives us following results (putting k = L in the first derived equation and k = L - 1 in the second derived equation)\n",
    "$$\\frac{\\partial J(A^{L})}{\\partial W^{L-1}} = \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(A^{L-1} \\right )^{T}$$\n",
    "$$\\frac{\\partial J(A^{L})}{\\partial b^{L-1}} = \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(\\Upsilon_{m,1} \\right )$$\n",
    "Which implies\n",
    "$$W^{L-1} = W^{L-1} - \\alpha \\cdot \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(A^{L-1} \\right )^{T}$$\n",
    "$$b^{L-1} = b^{L-1} - \\alpha \\cdot \\left [\\left(W^L \\right )^{T} \\times \\left [ \\frac{1}{m}\\frac{A^{L}-Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})\\right ] \\cdot G'^{L-1}(Z^{L-1})\\right ] \\times \\left(\\Upsilon_{m,1} \\right )$$\n",
    "This is already becoming too big to write, so let's derive the rest in a recursive manner.\n",
    "Let us assume that every layer is collecting some cost multipliers (things in the square brackets) that we'll denote by $\\delta ^{l}$ ($\\delta$ stands for *del*) (for *l*<sup>th</sup> layer).\n",
    "We'll define the following : \n",
    "$$\\delta ^{L} = \\frac{1}{m}\\frac{A^{L} - Y}{A^{L}(1-A^{L})} \\cdot G'^{L}(Z^{L})$$\n",
    "$$\\delta ^{L-1} = \\left[\\left(W^{L} \\right )^{T} \\times \\delta ^{L} \\right ] \\cdot G'^{L-1}(Z^{L-1})$$\n",
    "This gives the general result (for *l* being one of the _hidden_ layers)\n",
    "$$\\delta ^{l} = \\left[\\left(W^{l+1} \\right )^{T} \\times \\delta ^{l+1} \\right ] \\cdot G'^{l}(Z^{l})$$\n",
    "The updation of parameters of the *l*<sup>th</sup> layer is done by the following formula :\n",
    "$$W^{l} := W^{l} - \\alpha \\left[\\delta ^{l} \\times \\left(A^{l-1} \\right )^{T} \\right ]$$\n",
    "$$b^{l} := b^{l} - \\alpha \\left[\\delta ^{l} \\times \\Upsilon_{m,1} \\right ]$$\n",
    "If you just focus on the above five formulas, that's the main concept. We use _regularization_ to avoid the weights from gaining big values, and thus avoiding overfitting (it reduces variance). For that, we simply add the square of every parameter in the neural network, multiply by a value ($\\lambda$) and divide the entire result by $2m$. Thus, on differentiation, we get the $\\lambda \\frac{param}{m}$ term in the square brackets as well, so we multiply every parameter by $(1 - \\frac{\\alpha \\lambda}{m})$ in the training loop.\n",
    "This gives us the following results\n",
    "$$W^{l} := W^{l} \\left (1 - \\frac{\\alpha \\lambda}{m}\\right ) - \\alpha \\left[\\delta ^{l} \\times \\left(A^{l-1} \\right )^{T} \\right ]$$\n",
    "$$b^{l} := b^{l} \\left (1 - \\frac{\\alpha \\lambda}{m}\\right ) - \\alpha \\left[\\delta ^{l} \\times \\Upsilon_{m,1} \\right ]$$\n",
    "So let's first write code to get the $\\delta$ values for the entire network and then the change in weights $dW^{l}$ and biases $db^{l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation gradient calculation\n",
    "def backward_propagation_grads(cache, params, activations, Y):\n",
    "    \"\"\"\n",
    "    The backward propagation gradient generator\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Outputs\n",
    "    :return: return the gradients (changes to apply)\n",
    "        Dictionary\n",
    "            grads[\"dW\" + str(l)] : Gradient of weights of layer l\n",
    "            grads[\"db\" + str(l)] : Gradient of biases of layer l\n",
    "    \"\"\"\n",
    "    # Main variables\n",
    "    L = int(len(params.keys()) / 2)  # Number of layers in the neural network\n",
    "    m = cache[\"A0\"].shape[1]  # Number of training examples\n",
    "    # Calculate the multiplying factors\n",
    "    del_vals = {}             # The del values\n",
    "    if activations[L - 1] != sigmoid:  # To simplify calculations if the last layer has sigmoid activation\n",
    "        activation_derivative = activations[L - 1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m * cache[\"A\" + str(L)] * (1 - cache[\"A\" + str(L)]))) * \\\n",
    "                                   activation_derivative(cache[\"Z\" + str(L)])\n",
    "    else:\n",
    "        del_vals[\"del\" + str(L)] = ((cache[\"A\" + str(L)] - Y) / (m))\n",
    "    for l in range(L - 1, 0, -1):  # Go backward from layer L-1 to 1 to calculate del_vals[\"del\" + l]\n",
    "        activation_derivative = activations[l - 1]()[\"derivative\"]\n",
    "        del_vals[\"del\" + str(l)] = np.matmul(params[\"W\" + str(l + 1)].T, del_vals[\"del\" + str(l + 1)]) * \\\n",
    "                                   activation_derivative(cache[\"Z\" + str(l)])\n",
    "    # Calculate final derivatives\n",
    "    grads = {}\n",
    "    # Final backward step\n",
    "    for l in range(L, 0, -1):\n",
    "        grads[\"dW\" + str(l)] = np.matmul(del_vals[\"del\" + str(l)], cache[\"A\" + str(l - 1)].T)  # dW_l\n",
    "        grads[\"db\" + str(l)] = np.matmul(del_vals[\"del\" + str(l)], np.ones((m, 1)))            # db_l\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the final back propagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation step\n",
    "def back_propagation_deep(cache, params, activations, Y, learning_rate=0.01, reg_lambda=2):\n",
    "    \"\"\"\n",
    "    Perform one step of backward propagation\n",
    "    :param cache: Output and weghed sum of every neuron of every layer in the neural network\n",
    "            cache[\"A\" + str(l)] : Output of layer l\n",
    "            cache[\"Z\" + str(l)] : Weighed sum over the previous layer\n",
    "    :param params: The Weights and Biases of the neural network\n",
    "            params[\"W\" + str(l)] : Weights of layer l. From layer l-1 to layer l\n",
    "            params[\"b\" + str(l)] : Biases of layer l\n",
    "    :param activations: The list of activation functions of each layer\n",
    "    :param Y: Output\n",
    "    :param learning_rate: The learning_rate to use\n",
    "    :return: The new parameters of the neural network\n",
    "        Dictionary\n",
    "            params[\"W\" + str(l)] : The weights of layer l\n",
    "            params[\"b\" + str(l)] : The biases of layer l\n",
    "    \"\"\"\n",
    "    # Backward propagation step\n",
    "    L = int(len(params.keys()) / 2)\n",
    "    m = Y.shape[1]\n",
    "    grads = backward_propagation_grads(cache, params, activations, Y)\n",
    "    for l in range(L, 0, -1):  # Adjust gradients from layer L to 1\n",
    "        params[\"W\" + str(l)] = params[\"W\" + str(l)] * (1 - learning_rate * reg_lambda / m) \\\n",
    "                               - learning_rate * grads[\"dW\" + str(l)]\n",
    "        params[\"b\" + str(l)] = params[\"b\" + str(l)] * (1 - learning_rate * reg_lambda / m) \\\n",
    "                               - learning_rate * grads[\"db\" + str(l)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "Now let us define the performance metric function. I'm using the Chi squared cost function (it just finds the distance between the predictions and actual outputs in the *m* dimensional euclidian space, squares it, divides by 2 and then takes average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost_function(A_pred, Y):\n",
    "    \"\"\"\n",
    "    The Chi Squared cost function employed for knowing the goodness of fit\n",
    "    :param A_pred: Predictions made\n",
    "    :param Y: Actual output from the dataset\n",
    "    :return:\n",
    "        Cost\n",
    "    \"\"\"\n",
    "    diff_vect = A_pred - Y\n",
    "    diff_vect = np.square(diff_vect)\n",
    "    cost_val = np.average(diff_vect) / 2\n",
    "    return cost_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "We need to check how the neural network is performing on test data (some data that it has never seen while training).\n",
    "Let up implement this procedure as well with the help of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error test function\n",
    "def error_test_set(test_x, test_y, params):\n",
    "    A_pred, _ = forward_propagate_deep(params, activations, test_x)  # Get predictions from test data\n",
    "    predictions = np.zeros_like(A_pred)                              \n",
    "    predictions[A_pred > 0.5] = 1                                    # Threshold the predictions\n",
    "    diff_vector = predictions - test_y                               \n",
    "    diff_vector = np.square(diff_vector)                             # Examples where the predictions were wrong\n",
    "    mismatch_vector = diff_vector.nonzero()[1].reshape((1, -1))      # Index of mismatches\n",
    "    print(\"TEST REPORT : {mis} mismatches out of {tot} test cases\".format(mis=mismatch_vector.shape[1],\n",
    "                                                                          tot=test_y.shape[1]))\n",
    "    print(\"TEST REPORT : {err}% mismatch error\".format(err=mismatch_vector.shape[1] / diff_vector.shape[1] * 100))\n",
    "    for i in range(mismatch_vector.shape[1]):\n",
    "        x_view = test_x[0,mismatch_vector[0,i]]\n",
    "        img = x_view.reshape((47, 38))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Y = {y}, A = {a}\".format(y=test_y[0,mismatch_vector[0,i]], \n",
    "                                            a=predictions[0,mismatch_vector[0,i]]))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
